---
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "",
  fig.width = 6,
  fig.height = 6,
  # eval = FALSE,
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```

<!--                              Notes/Andrew:                              -->
<!------------------------------------------------------------------------------
I feel like there should be at least a some code demonstrating basic download
with a couple of the other mentioned packages, like spocc, and then moving on to
galah specific. Thoughts?

Changes: This section had a confusing structure to me, so I've changed it to
reflect how I think it should flow but its still a work in progress. Before, the
first chunk of code that demonstrated downloading data was closely tied to
taxonomy and naming discrepancies, which I think it's a bit much going on for a
first demonstration of downloading. I think an inverted pyramid appraoch should
be followed here (and in general across the book) - start with simple downloads,
and then building up to more complex queries from there. Following the
fundamentals, thats when I think you can show the examples that relate to or
showcase more specific issues (e.g. taxonomic name variation etc, like the
orchid example), can go. This makes the more basic instructions and code
demonstrations clearer and also easier to find and use, as a quick reference,
and for cross references, rather than having only complex examples or examples
that showcase more than one concept or functionality, which can be harder to
index and cross reference.
------------------------------------------------------------------------------->

# Download data {#sec-download-data} 

Having decided on the scope of data required, we can get started with
downloading data. We will discuss where and how to get data, as well as
different techniques to refine your downloads.

## Where to get data from

There are a wide variety of data aggregators available online, ranging from more
regional or localised records, to those with a global scope. We will introduce a
few of the most common ones. 

One of the largest biodiversity aggregators is the [Global Biodiversity
Information Facility (GBIF)](https://www.gbif.org/), an international network
and data infrastructure that provides open access biodiversity data from many
sources around the world. Presently, GBIF manages and serves over 1.5 billion
occurrence data points. These data points are aggregated from a number of
particpant organisations, each of which collate biodiversity data from their own
regions and sources, to be distributed through a designated 'node', with GBIF
acting as the overarching organisation to store and provide this data using a
unified data standard. 

In addition to GBIF, there are a variety of other well known aggregators that index a
large amount of records, such as [Integrated Digitzed
Biocollections](https://www.idigbio.org) and
[VertNet](https://www.vertnet.org/). 

Besides these large scale aggregators, we can also download data dierectly from
more localised aggregators, such as the regional nodes of GBIF. For example, the
[Atlas of Living Australia (ALA)](www.ala.org.au) is the Australian node,
[Sistema de Informação sobre a Biodiversidade Brasileira
(SiBBr)](https://www.sibbr.gov.br/) is the Brazilian node, and [GBIF
Sweden](https://www.gbif.se/) is the Swedish node. Living Atlases like the ALA
ingest and aggregate data from a broad range of providers such as government
monitoring programs, museums and herbaria, research projects and citizen science
initiatives. If your project is focused on a specific region, downloading data
directly from a regional node may be more appropriate.

::: {.callout-note}
To see what national and regional nodes exist, check out [The GBIF Network](https://www.gbif.org/the-gbif-network).
:::

If your project relates to data from a specific data provider, it also might be
best to download data directly from the source. For example, a common citizen
science tool to collect species observations is
[iNaturalist](https://www.inaturalist.org/). Downloading directly from the
original data source can help to ensure you don't have any stray data from other
sources.

## Packages for downloading data

There are a range of R packages available for accessing biodiversity data. These
packages serve as convenient interfaces to various data providers. By wrapping
the respective APIs of data providers, they streamline the process of accessing
and downloading datasets, directly within R. The functionality offered by these
packages typically ranges from querying species occurrence records, to more
comprehensive taxonomic and spatial download queries. Below, we highlight some
commonly used packages. We encourage users to explore the documentation of each
package to understand their capabilities, which will help you select one (or
more!) that align with your specific needs.

- [rgbif](https://docs.ropensci.org/rgbif/) - Search and retrieve data from the
  Global Biodiversity Information Facility (GBIF)
- [galah](https://galah.ala.org.au/index.html) - An interface for accessing GBIF
  and GBIF network nodes that maintain their own APIs (i.e. the [‘living
  atlases’](https://galah.ala.org.au/R/articles/choosing_an_atlas.html))
- [rinat](https://docs.ropensci.org/rinat/) - An R wrapper for accessing
  [iNaturalist](https://www.inaturalist.org) observations
- [rebird](https://docs.ropensci.org/rebird/) - Provides access to the
  [eBird](https://ebird.org/home) webservices.
- [spocc](https://docs.ropensci.org/spocc/) - Query and collect species
  occurrence data from a variety of sources, including GBIF, ALA,
  [VertNet](https://github.com/ropensci/rvertnet),
  [iDigBio](http://www.idigbio.org/) and others.

## Basic downloads via `galah`

In the majority of examples we will be using the `galah` package for download
queries. One benefit of using `galah` is that in addition to species occurrence
records, we can specify what metadata to return, from all available fields. It
also allows access to any media, such as images or sounds, associated with the
occurrence records. 

### Setting an atlas

With `galah`, records are downloaded from the configured atlas. You can set the
atlas using `galah_config()`. For example, to set the atlas to the Global
Biodiversity Information Facility (GBIF), enter your GBIF account credentials
and set the atlas to "Global". See `?galah_config` for more configuration
options. You can enter your details directly, or else [save
them](https://docs.ropensci.org/rgbif/articles/gbif_credentials.html) in your
`.Renviron`, so you don't have to enter them explicitly. 

```{r, echo = TRUE}
library(galah)
galah_config(
  atlas    = "Global",
  email    = Sys.getenv("GBIF_EMAIL"),
  username = Sys.getenv("GBIF_USER"),
  password = Sys.getenv("GBIF_PWD")
)
```

### Download from GBIF

For a basic download query, we simply pass a taxonomic identifier to the
`galah_identify()` function, and end our query with `atlas_occurrences()` to
return a data frame from the query. This is similar to the queries we made in
@sec-temporal-scope, except there we used `atlas_counts()` to return counts of
records rather than the observation data itself. Below is an example using the
species name of the African Elephant, *Loxodonta africana*. 

::: {.callout-note}
Depending on your connection speed, downloads such as this (> 1700 records) may
take some time to complete 
:::

```{r, echo = TRUE}
african_ele <- galah_call() |>
  galah_identify("Loxodonta africana") |>
  atlas_occurrences()
```

Downloaded records can be saved locally using `write.csv()` or
`readr::write_csv` as with any other dataframe. For larger downloads, we
recommend saving the data as a Parquet file, as the compression and read/write
speeds are typically better for this type of data structure. We can do this
using the `arrow::write_parquet()` function, and `arrow::read_parquet()` can be
used to read in a parquet file.

```{r, echo = TRUE}
arrow::write_parquet(african_ele, "data/gbif/elephant")
elephant <- arrow::read_parquet("data/gbif/elephant")
```

### Download from a regional node

Above, we downloaded records from GBIF. To access data from the Australian node,
we change the `galah` configuration so that our query points to Australia. After
that, we will download all records for the Pink Robin.

```{r, echo = TRUE}
galah_config(
  email = Sys.getenv("ALA_EMAIL"),
  atlas = "Australia"
)

pink_robin <- galah_call() |>
  galah_identify("Petroica rodinogaster") |>
  atlas_occurrences()
```

### Return specific data fields

When downloading data using `atlas_occurrences`, a default selection of data
fields (columns) are returned, which includes some of the key taxonomic and
spatial fields. There are fields for a wide array of metadata that may accompany
an observation. To choose what fields are included in a download query, the
[`galah_select`](https://galah.ala.org.au/R/reference/galah_select.html)
function can be used. This function takes a vector of field names as an
argument, and when used within a query, a tibble with only those fields will be
returned. To see the fields available for selection use `show_all(fields)`. 

Below is an example of using `galah_select()` to download only the requested
data fields, for records of the species `Petroica rodinogaster`. 

```{r, echo = TRUE}
library(galah)
galah_config(email = Sys.getenv("ALA_EMAIL"))
project_fields <- c(
  "recordID",
  "eventDate",
  "year",
  "basisOfRecord",
  "occurrenceStatus",
  "scientificName",
  "genus",
  "decimalLatitude",
  "decimalLongitude"
)
pink_robin_projfields <- galah_call() |>
  galah_identify("Petroica rodinogaster") |>
  galah_select(all_of(project_fields)) |>
  atlas_occurrences()
```

## Taxonomic queries

Techniques specific to taxonomic queries and related knowledge. 

### Name discrepancies

<!-- Andrew: what does this mean, needs some context -->
To search by a taxonomic classification, include the original name that was
recorded by the data provider in your download.

```{r}
# Orchid as an example
# Read in problem child data and show
# Group by species and count unique values for higher taxonomy
```

Orchids (family *Orchidaceae*) are one taxonomic clade that naming authorities
have (famously) differing opinions about taxonomic classification. Comparing the
scientific name originally provided with the observation by the data provider
(i.e. `raw_scientificName`) compared to the name held within the Atlas of Living
Australia (i.e. `scientificName) we are likely to notice some differences.

For example, say we were interested in downloading observations of *Dendrobium*,
one of the largest genera of Orchids. Let's download occurrence data of this
genus using galah, and add `raw_scientificName` & `taxonomicStatus` information
columns to our download.

```{r}
galah_config(
  atlas   = "Australia",
  email   = Sys.getenv("ALA_EMAIL")
)

# Download Dendrobium occurrences
orchids <- galah_call() |>
  galah_identify("Dendrobium") |>
  galah_select(group = "basic", raw_scientificName, taxonomicStatus) |>
  atlas_occurrences()
```

If we filter our data to data without an "accepted" taxonomic status and compare
a random group of names (rows 30 to 50), we'll see a few names with differing
names between the `raw_scientificName` and `scientificName`

<!-- Andrew: I got the message "new names", telling me to use scientificName...4
and ...9? From the download query? No idea what that is about, but had to change
the code below from scientificName and raw scientific name to those names, or it
wouldn't run for me -->

```{r}
library(dplyr)
orchids |>
  filter(
    is.na(taxonomicStatus), # return names that aren't flagged as "accepted"
    !scientificName...4 %in% scientificName...9
  ) |>
  select(scientificName...4, scientificName...9) |>
  slice(30:50)

```

Lucky for us, the Atlas of Living Austrlaia accounts for some of these
differences when storing their data to help standardise these data
taxonomically. However, this isn't always the case for every taxonomic clade.
Searching by classification is a good first step when you expect to find certain
taxonomic classifications within your data set.

It's useful to include synonyms of the species names you're interested in your
download to ensure you return the data you are interested in.

```{r}
#| eval: true
#| echo: true
search_taxa("Dendrobium keffordii")
search_taxa("Grastidium baileyi")

## Not sure what was meant to happen here
library(galah)

galah_call() |>
  galah_filter(year > 2019) |>
  atlas_counts()
```

## Spatial filtering

<!-- Fonti notes:-->
<!-- Work in progess -->
<!-- Narrowing your results to a specific spatial area- using `galah_geolocate`
or by regions etc, discussions around whether this is best to be placed in this
chapter to assist with expaning scope has been raised.  -->
<!-- Read in shpfile, transform to polygon -->

We can add spatial filters to download queries, to return only records within a
specified area. Spatial filtering  , and are useful for long-term analyses of
observations in specific regions or areas.

### Filter by region

One way to download data is by filtering to an area of interest using fields
already in the ALA and {galah}.

```{r}
search_fields("wetlands")
search_fields("cl901") |>
  search_values("kakadu")

# Northern Snake-necked Turtle
galah_call() |>
  galah_identify("Chelodina oblonga") |>
  galah_filter(cl901 == "Kakadu National Park") |>
  atlas_occurrences() |>
  head(5) |>
  gt::gt()

```

```{r}
# Show how to download data in galah with a bounding box
```

### Filter by vector geometry

Vector geometry can also be used to filter observations. The advantage of this
method is that you can return data for very specific shapes or areas. The
`galah_geolocate()` function accepts vector geometries in the form of simple
feature objects, shapefiles or Well-Known Text (WKT) strings. 

In this example, we first construct a simple polygon for a theoretical "site A",
using a WKT string. You could also import an existing geometry such as a
shapefile. To work with spatial vector data we use the `sf` package. The
function `st_as_sf()` is used to create a simple feature with type polygon, from
our WKT string. The coordinate reference system (CRS) of our WKT is 4326, so we
need to use `st_set_crs()` to set this value for our simple feature object. Now
we can use this object in a download query using `galah_geolocate()` to filter
the records. 

```{r}
my_polygon <- dplyr::tibble(
  site = "A",
  geometry = "POLYGON((149.96704 -32.08651, 150.64294 -32.2957, 151.18152 -32.58776, 150.98376 -33.10617, 149.86316 -32.83581, 149.59900 -31.90793, 149.96704 -32.08651))"
)

my_polygon_sf <- sf::st_as_sf(my_polygon, wkt = "geometry") |>
  sf::st_set_crs(4326)

# Satin Bowerbird (*Ptilonorhynchus violaceus*
satin_bb <- galah_call() |>
  galah_identify("Ptilonorhynchus violaceus") |>
  galah_geolocate(my_polygon_sf) |>
  atlas_occurrences()
```

Our download query returns only the records from within our polygon. We can
visualise the result using `ggplot2`:

```{r}
library(ggplot2)

aus <- sf::st_transform(ozmaps::ozmap_country, 4326)

ggplot() +
  geom_sf(data = aus) +
  geom_sf(data = my_polygon_sf, fill = "springgreen", alpha = 0.2) +
  geom_point(data = satin_bb, color = "orchid1", aes(
    x = decimalLongitude,
    y = decimalLatitude
  )) +
  coord_sf(xlim = c(148, 153), ylim = c(-34, -31)) +
  theme_classic() +
  theme(legend.position = "bottom")
```

### Filter by bounding box

Another way to filter a query is by using a bounding box. This is similar to the
polygon method shown above. We again use `galah_geolocate()`, but set the type
arguement, which is polygon by default to "bbox". As a result, the provided
POLYGON or MULTIPOLYGON will be converted into the smallest bounding box
(rectangle) that contains the POLYGON. In this case, records will be included
that may not exactly lie inside the provided shape.

## Refining your download query

Open access biodiversity data comes from many different data sources such as
government monitoring programs, museums, herbaria, research projects, and
citizen science apps. As such, data type and quality can vary considerably. For
example, museums harbour older records that are associated with a preserved
specimens. These data often contain lots of extra information (metadata) about a
specific specimen and its location. On the other hand, data sourced from citizen
science apps like iNaturalist or eBird may have less extensive metadata in
comparison, but can include associated images or sounds.

Refining your download query is useful for downloading only records that meet
your project requirements, or filting records that may be of a lower quality. In
any case, refining a dowload query where possible has the benifit of reducing
filtering you would otherwise need to perform locally, and also reducing
download size. Below we have illustrate a few ways of refining your download
query using `galah_filter`. 

### By Year

Generally, old data records tend to be insufficient or less reliable as
taxonomic knowledge and GPS tools were not readily available. For this reason,
many users consider removing all occurrence records before a certain year to
increase data precision [@gueta_quantifying_2016; @marsh_accounting_2022]. 

Choosing the year 'cut-off' is relatively arbitary, but the most commonly used
year is 1945 [@zizka_no_2020; @fuhrding-potschkat_influence_2022], although some
studies discard all data collected before 1990 [@gueta_quantifying_2016;
@marsh_accounting_2022].

Here we will narrow the Pink Robin query from above to records after 1945 using
`galah_filter`:

```{r, echo = TRUE}
pink_robin_post1945 <- galah_call() |>
  galah_identify("Petroica rodinogaster") |>
  galah_filter(year > 1945) |>
  atlas_occurrences()
```

### Basis of record

Basis of record is a [Darwin Core
term](https://dwc.tdwg.org/terms/#dwc:basisOfRecord) that refers to the specific
nature of the occurrence record. It can be used to refine your data download and
ensure consistency when consolidating data from multiple organisations
[@fuhrding-potschkat_influence_2022]. 

There are 6 different classes for basis of record: 

- Living Specimen - a specimen that is alive, e.g. a living plant in a national park
- Preserved Specimen - a specimen that has been preserved, for example, a dried plant on an herbarium sheet 
- Fossil Specimen - a preserved specimen that is a fossil
- Material Sample - a genetic or environmental sample
- Material Citation - A reference to, or citation of, a specimen in scholarly publications, e.g a citation of a physical specimen in a scientific journal 
- Human Observation - an output of human observation process e.g. evidence of an occurrence taken from field notes or an occurrence without any physical evidence
- Machine Observation - An output of a machine observation process e.g. a photograph, a video, an audio recording, a remote sensing image or an occurrence record based on telemetry.

Depending on your data scope, it may be practical to limit data that can be
traced to a physical specimen or observation [@godfree_implications_2021], which
we do for the Pink Robin below

```{r, echo=TRUE}
tractable_records <- c(
  "LIVING_SPECIMEN",
  "PRESERVED_SPECIMEN",
  "MATERIAL_SAMPLE",
  "MACHINE_OBSERVATION"
)

pink_robin_tractable <- galah_call() |>
  galah_identify("Petroica rodinogaster") |>
  galah_filter(basisOfRecord == tractable_records) |>
  atlas_occurrences()
```

### Assertions

Data infrastructures use assertions to internally grade the quality,
completeness and consistency of each occurrence record. Assertions take values
of either 1 or 0, indicating the presence or absence of the data quality issue.
Note that assertions may vary depending what atlas you have configured to. You
can see the available assertions and their descriptions using:

```{r, eval = TRUE, echo=TRUE}
show_all("assertions")
```

Once you have decided which assertions are important for your project you can
further refine your download. To retrieve all the assertions for your query use
`galah_select(group = "assertions")`

<!-- Explain:  -->
<!-- Assertions are logical flags, TRUE is presence of issue denoted in column name -->
<!-- User to choose which ones are relevant to their project and filter in query -->
<!-- For exclusions using multiple assertions (Currently throws errors, need to think carefully about != means for logical assertions...) -->

```{r}
probin_assertions <- galah_call() |>
  galah_identify("Petroica rodinogaster") |>
  galah_filter(basisOfRecord == tractable_records) |>
  galah_select(group = "assertions") |>
  atlas_occurrences()

# Preview all the assertions
head(colnames(probin_assertions), 10)

# A quick way to check which assertions contain TRUEs
probin_assertions |>
  select(-recordID) |>
  colSums()

# Requery for single assertion
probin_subset <- galah_call() |>
  galah_identify("Petroica rodinogaster") |>
  galah_filter(
    basisOfRecord == tractable_records,
    identificationIncorrect == FALSE
  ) |>
  galah_select(group = "basic") |>
  atlas_counts()


# For exclusions using multiple assertions [Currently throws errors, need to think carefully about != means for logical assertions...]
# assertions <- c("UNKNOWN_KINGDOM", "identificationIncorrect",
#                    "COORDINATE_PRECISION_MISMATCH", "MISSING_GEODETICDATUM")
#
# galah_call() |>
#   galah_identify("Petroica rodinogaster") |>
#   galah_filter(basisOfRecord == tractable_records,
#                assertions != IA_assertions) |>
#   atlas_counts()
```
