---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, setup, include=FALSE}

knitr::opts_chunk$set(
  comment = '', 
  fig.width = 6, 
  fig.height = 6,
  eval = FALSE,
  echo = FALSE,
  warning = FALSE
)

options(tidyverse.quiet = TRUE)

library(galah)
library(dplyr)
library(arrow)
library(skimr)
library(janitor)
```

# Precleaning

Precleaning prepares the dataset in a general manner so that it is formatted in a logical and consistent manner. It is a 'broad sweep' procedure that allows you to familiarise with the data but it also makes the next stage of in-depth data cleaning proceed more smoothly [@streamdna_sharing_2020]. We will discuss some approaches on how to be curious with your data and how to detect and handle string inconsistencies, missing data and outliers.

## Metadata

Metadata describes your dataset. It defines each variable and describes its contents such as what units a variable is measured in. Data infrastructures that uses Darwin Core terms will have interoperable metadata. All Darwin Core term definitions can be found [here](https://dwc.tdwg.org/terms/), we suggest using `Ctrl/CMD F` and searching your variable name on the [webpage](https://dwc.tdwg.org/terms/). Don't hesitate to Google variable names if you are unsure what they represent.

If you're using data from particular institutions it's worth checking the metadata for the entire dataset to delineate if there is extra information about the data which might limit what you can do with the data. You could google the dataset name, or search the dataset/institution on the ALA. The metadata on the ALA is submitted with the data, it's not always of high standard which is why it's worth investigating externally, an example of some good metadata is [FrogID from the Australian Museum](https://collections.ala.org.au/public/show/dr14760). A benefit to reading metadata is understanding the limitations of your dataset. From reading FrogID's metadata [@rowley2020frogid], you'll find:

1.  The data is acoustic data, the majority of the species recorded are therefore male
2.  Because this is citizen science data, it is especially biased towards populated areas
3.  Audio is reocrded via a shartphone app, the authors recommend if you require high coordinate precision to filter data to `geographic uncertainty of <3000m`
4.  The data is presence only data

Metadata can also be useful for understanding the license that the data falls under, this is mostly relevant for using multimedia associated with the data.

## Initial inspection

A great way to get an initial overview of your data is to use the R package `skimr`. Importantly `skimr` produces tables of descriptive statistics, such as amount of missing data, for every variable

The output is also grouped by data type (numeric, character, date) so you can also check for any inconsistencies. As you are looking through the output, ask yourself whether the data is in line with your expectation. If you requested data for a group of species, are they all represented? Are the values for a variable reasonable? Looking at the quartiles will help you get the sense of the distribution of data. These considerations will help you detect potential issues in the data.

```{r, echo = TRUE}
library(skimr)

skim(african_ele)
```

Here is the `skimr` report for our African elephant dataset we [downloaded earlier](download-data.qmd)

```{r, eval = TRUE}
african_ele <- read_parquet("data/gbif/elephant")

skim(african_ele)
```

## String inconsistencies

String inconsistencies include mispellings, capitalisation errors, misplaced punctuations or trailing white spaces. We will use the `janitor` R package to explore whether our data has any of these issues. The function `tably` will compute a counts and percent of total rows for each unique value.

We recommend `tabyl-ing` any character strings that are relevant to your project. For example, here is the [`stateProvince`](https://dwc.tdwg.org/terms/#dwc:stateProvince) in alphabetical order.

```{r, echo = TRUE}
library(janitor)

african_ele %>%
  pull(stateProvince) %>% 
  tabyl() %>% 
  tibble() %>% 
  print(n = 20)
```

```{r, eval = TRUE}
african_ele %>%
  pull(stateProvince) %>% 
  tabyl() %>% 
  tibble() %>% 
  print(n = 20)
```

From the `tabyl` output, we can see there are few different variations of `Province`, `Prov.`, `Prov`. As an example, we will correct these with the `tidyverse` packages `stringr`, `dplyr`, `tidyr` as well as `glue`. If you are not very familiar with regular expressions, we highly recommend this [cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

```{r, eval = TRUE, echo = TRUE}
library(tidyverse)
library(glue)

# Create a regular expression to match Prov. and Prov
# The pattern below means Prov that is NOT followed by any lowercase letters
pattern = regex("Prov(?![:lower:])")

# Use `str_subset` to pull out the cases that match our pattern
# Confirm that these are the problematic ones
# Assign these into an object
str_subset(african_ele$stateProvince, pattern = pattern)
typos_provinces <- str_subset(african_ele$stateProvince, pattern = pattern)

# Create a new variable `stateProvince_clean` using `mutate`, `if_else`, `str_detect` and `glue`
# `str_detect` will evaluate values of `stateProvince` that matches our pattern we defined earlier.
# Matches will return TRUE, non-matches will return FALSE. 
# The `if_else` will then evaluate these logicals (TRUE/FALSE/NA) 
# for TRUE values, the `glue` function will take the first part of the province name enclosed in and join it with word Province.
# for FALSE values , it will just take the corresponding value in stateProvince
# Note that we are assigning these changes to a new object (`african_ele_2`)
african_ele_2 <- african_ele %>% 
  mutate(stateProvince_clean = if_else(str_detect(stateProvince, pattern = pattern),
                                      true = glue('{word(stateProvince, sep = " P")} Province'),
                                      false = stateProvince)
         ) 

# Once we've made the correction we want to check we've done it correctly.
# ALWAYS CHECK YOUR CORRECTIONS
# Use the `select` function to isolate columns that `starts_with` "stateProvince"
# Use the `filter` function to subset our the problematic provinces 
african_ele_2 %>% 
  select(starts_with("stateProvince")) %>% 
  filter(stateProvince %in% typos_provinces)

# Its good practice to check the other values were not affected by your corrections
# Here we are removing the NA with `drop_na` and subsetting unique rows with `distinct`
african_ele_2 %>% 
  select(starts_with("stateProvince")) %>% 
  drop_na() %>% 
  distinct() 

# Final check
# Check with the original code that detected the issue
african_ele_2 %>%
  pull(stateProvince_clean) %>% 
  tabyl() %>% 
  tibble() %>% 
  print(n = 20)
```

There are some other issues that can be corrected in a similar approach:

-   `North West`, `North West District` and `North-Western`
-   `Ã€frica Central`, `Central Province` and `Central`
-   `Atacora` and `Atakora`
-   `Coastal Province` and `Coastal`

We recommend consulting reputable sources that can help delineate or consolidate similar values. Googling and looking at Wikipedia's sources are good places to find resources that you can verify accepted state and province names.

## Missing data

(I wonder if this is really the place for this or better to just do this in the Spatial chapter)

-   Remove records with no coordinates

## Quick visualiations

A graphic plot of your data can be very telling and can help you spot potential errors that may be due to formatting.

### GGally

A visual inspection of your entire dataset can save time and solve easy-to-spot errors.

### Quick map

(I wonder if this is really the place for this or better to just do this in the Spatial chapter)

A simple way to visualize your data is to plot it on a map.

-   Fix minor coordinates errors, such as inverted or badly formatted

```{r}

```

### 
