---
editor: 
  markdown: 
    wrap: 72
---

<!-- I think my general comments / proposed changes for this chapter just involve visual cues to different parts so skimming the doc is a little easier! OT :-)  --> 

# Duplicates & overlaps

**Duplicate** records are when there are multiple occurrences of the the same entry in a dataset. They can happen in individual datasets (e.g. due to collection or entry error), but are even more likely when using aggregated data sources. Without removing them, any analyses or visualisations using your data may be misleading. Duplicates can give the impression that there are more data than there really are and bias your analyses to favour certain species, locations or time periods.

<!-- this bit below refers to something to me that isn't a duplicate, but the steps followed for a duplicate will be similar. i gave it the name overlap in the title because someone might be looking specifically for steps to deal with that, and won't necessarily clock on duplicate? OT --> 

These are different to **overlapping** records, but similar steps to duplicate record cleaning are used to alleviate them. Overlap as a concept depends on the analysis you wish to run. Usually, overlap occurs when there are **separate** observations of a species, but the records overlap taxonomically, spatially or temporally, and their overlap would potentially bias a result. Prior to running species distribution models, for example, you often need to remove duplicate records that overlap within a certain radius of the same location in order to avoid bias in your model.

In this section we will cover detection and handling of **spatially** duplicate records as we are working with biodiversity occurrence data. There are relevant coding steps for adressing overlapping records in this chapter, too. <!-- if you want to we could add sections on taxonomic and temporal overlap but isn't really necessary I guess OT --> 

### Prerequisites

For this demonstration we use occurrence records for kingfishers (*Alcedinidae*) since 2023.

Here's how we download them using [galah](https://galah.ala.org.au/). 

```{r prereq}
#| message: false
#| warning: false
#| echo: false
library(galah)
library(dplyr)
library(janitor)
galah_config(email = Sys.getenv("ALA_EMAIL"),
             verbose = FALSE)
birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", 
         family, genus, species, cl22, eventDate, month) |>
  atlas_occurrences()
```

```{r}
#| eval: false
# packages
library(galah)
library(dplyr)
library(janitor)

# data: Kingfisher records from 2023
galah_config(email = "your-email-here") # ALA Registered email

birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", 
         family, genus, species, cl22, eventDate, month) |>
  atlas_occurrences()
```

:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/2/5/1/d/9525c6da-fd47-41fa-8eff-55c6747ed152/original"></img>

::: {.figure-caption}
[*Todiramphus (Todiramphus) sanctus* perched on a branch. Photo by Kerri-Lee Harris CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/77b8aac0-18af-4ec6-a03c-ff825859a6f3)
:::
:::

## Find spatial duplicates

Let's remove any records that have multiple occurrences in the same location specified by their latitude and longitude coordinates.

The first thing to do is find the duplicate records.

:::{.panel-tabset .nav-pills}
#### dplyr

Return a summary of the number of duplicates with each set of coordinates.

```{r}
birds |> 
  group_by(decimalLongitude, decimalLatitude) |>
  filter(n() > 1) |>
  summarise(n = n())
```

Return a summary of duplicate decimal longitude and latitude rows in the entire dataset. 

```{r}
birds |>
  filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
```

#### janitor

<!-- I think janitor uses commas like an OR statement, so this returns more rows than dplyr -->
<!-- Worth including? -->

Return duplicated rows and the number of duplicates of `decimalLatitude` OR `decimalLongitude` (note that this differs from the dplyr example).

```{r}
birds |> 
  get_dupes(decimalLatitude, decimalLongitude)
```

:::
::: {.callout-important title="Data overlaps"}
In the above `tibble` our results show that there are just over 27,000 records that **overlap** spatially (at the same geographic coordinates).
:::

Note that while we want to remove duplicate records, we don't necessarily want to remove *all* duplicates. Our duplicate dataset contains several different species, and sometimes, several species might be in the same location. Rather than removing all but one species in a location, we probably want to remove duplicates *for each species*. This should leave one observation for each species in each location.

To filter our duplicate data by species, we can first split our data by species...

```{r}
#| eval: false
birds |>
  group_split(species)
```

```{r}
#| class: output-scroll
#| echo: false
birds |>
  group_split(species)
```


...and use `purrr::map()`[^1] to remove duplicates for each species group, binding our dataframes together again with `bind_rows()`.

[^1]: We have used `\(df)` as shorthand within `purrr::map()`. This shorthand can be rewritten as `map(.x = df, function(.x) {})`.<br><br> We provide an input, in this case the piped dataframe which we've called `df`, and use it in a custom function (defined within `{}`). This function is run over each dataframe in our list of dataframes.<br><br>Check out [this description from a recent purrr package update](https://www.tidyverse.org/blog/2022/12/purrr-1-0-0/#documentation) for another example.

```{r}
library(purrr)

birds |>
  group_split(species) |>
  map(\(df) 
      df |> 
        filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
      ) |>
  bind_rows()
```

Our final data of duplicate records in our dataset looks like this. 

```{r}
#| echo: false
#| output: true
birds |>
  group_split(species) |>
  map(\(df) 
      df |> 
        filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
      ) |>
  bind_rows() |>
rmarkdown::paged_table()
```


::: {.callout-important title="Splitting"}
In the `tibble` above, you can see that splitting overlaps by species has reduced the total number of duplicate records by ~3,500 rows.
:::

## Remove spatial duplicates

To now *remove* these duplicates from our dataframe, we can use the `!` operator to return records that *are not* duplicated, rather than those that are.

<!-- does the map function... save the previous two lines of code and then make it possible to further filter within the same line? im just guessing, i don't find it super clear or from what package it comes from. maybe worth signposting but also not super necessary if i was able to figure that out i guess? OT --> 

```{r}
birds_filtered <- birds |>
  group_split(species) |>
  map(\(df) 
      df |>
        filter(!duplicated(decimalLongitude) & !duplicated(decimalLatitude))) |>
  bind_rows()
birds_filtered
```

To check our results, we can grab a random row from our unfiltered dataframe...

```{r}
test_row <- birds |>
  filter(duplicated(decimalLongitude) & duplicated(decimalLatitude)) |>
  slice(10)

test_row |>
  select(species, decimalLatitude, decimalLongitude, recordID) # show relevant columns
```

...and see whether any rows in `birds_filtered` have the same combination of longitude and latitude coordinates.

```{r}
birds_filtered |>
  filter(
    decimalLatitude %in% test_row$decimalLatitude & 
      decimalLongitude %in% test_row$decimalLongitude
    ) |>
  select(species, decimalLatitude, decimalLongitude, recordID) # show relevant columns

```

As expected, there are a few species with those latitude and longitude coordinates, but we now only have 1 row for each species in that location in `birds_filtered`. 

Using `%in%` can be a powerful tool for finding duplicates in your dataframe. Extracting rows like we did above with our `test_row` example above (or a list of values in a column) can help you weed out more specific duplicate records you are interested in.


...Now we have our dataset of kingfishers cleaned for spatially duplicate records!

```{r}
#| echo: false
birds_filtered |>
  rmarkdown::paged_table()
```

:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/d/5/4/b/cec90fd8-8d85-43aa-9d3a-71c3e73db45d/original"></img>

::: {.figure-caption}
[*Ceyx azureus* perched on a log. Photo by andrewpavlov CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/ccba93a8-92e2-4291-ad72-24b9553258f0)
:::
:::

## Summary

This chapter has offered a quick look at how to find duplicate records, remove them from your dataset, and check whether your records have been removed correctly. It has also provided insight into working with spatially overlapping records. You may have to check for duplicates in several ways to ensure you have cleaned all the duplicate records that you wish to remove from your dataset. The next chapter will show you how to clean **missing values** from your dataset.
