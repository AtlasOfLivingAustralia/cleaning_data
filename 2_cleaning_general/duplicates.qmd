---
editor: 
  markdown: 
    wrap: 72
---

# Duplicates

Duplicate records can occur for a number of reasons. A duplicate record might appear in an individual dataset due to collection or entry error. A record might be duplicated when aggregating data sources. Alternatively, a record might be considered a duplicate in the context of one type of analysis, but not another. For example, prior to running species distribution models, records in the same location---even if they are separate observations---are considered duplicates and should be removed to avoid spatial bias. If running multiple models for several time-periods, however, records in the same location may be permitted in the final dataset if they occurred in different time-periods. *Context is key when determining how to identify and clean duplicate records in your dataset.*

Understanding how to replace duplicates is important because skipping this step could result in misleading analyses or visualisations. Duplicates can give the impression that there are more data than there really are and bias your analyses to favour certain species, locations or time periods. 

In this chapter we will introduce detection and handling of duplicate records as we are working with biodiversity occurrence data.

:::{.callout-tip collapse="true"}

The methods used in this chapter to search and clean duplicate records can be applied to other types of data columns, too, not only columns that contain spatial data. 

Depending on your analysis, you might need to use more bespoke methods to clean duplicates. Later chapters like [Taxonomic validation](../3_cleaning_expert/taxonomic-validation.qmd) and [Geospatial cleaning](../3_cleaning_expert/geospatial-cleaning.qmd) cover more advanced detection and cleaning methods.

:::

### Prerequisites

In this chapter, we will use kingfisher (*Alcedinidae*) occurrence data in 2023 from the ALA.

```{r prereq}
#| message: false
#| warning: false
#| echo: false
library(galah)
library(dplyr)
library(janitor)
galah_config(email = Sys.getenv("ALA_EMAIL"),
             verbose = FALSE)
birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", 
         family, genus, species, cl22, eventDate, month) |>
  atlas_occurrences()
```

```{r}
#| eval: false
# packages
library(galah)
library(dplyr)
library(janitor)

# data: Kingfisher records from 2023
galah_config(email = "your-email-here") # ALA Registered email

birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", 
         family, genus, species, cl22, eventDate, month) |>
  atlas_occurrences()
```


:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/2/5/1/d/9525c6da-fd47-41fa-8eff-55c6747ed152/original"></img>

::: {.figure-caption}
[*Todiramphus (Todiramphus) sanctus* perched on a branch. Photo by Kerri-Lee Harris CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/77b8aac0-18af-4ec6-a03c-ff825859a6f3)
:::
:::


## Find duplicates

Let's remove any records that have multiple occurrences in the same location specified by their latitude and longitude coordinates.

The first thing to do is find the duplicate records.

:::{.panel-tabset .nav-pills}
#### dplyr

Return a summary of the number of duplicates with each set of coordinates.

```{r}
birds |> 
  group_by(decimalLongitude, decimalLatitude) |>
  filter(n() > 1) |>
  summarise(n = n())
```

Return a summary of duplicate decimal longitude and latitude rows in the entire dataset. 

```{r}
birds |>
  filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
```

#### janitor

Return duplicated rows and the number of duplicates of `decimalLatitude` OR `decimalLongitude` (note that this differs from the dplyr example because janitor uses commas as an OR statement).

```{r}
birds |> 
  get_dupes(decimalLatitude, decimalLongitude)
```

:::

In the above `tibble` our results show that there are just over 27,000 records that overlap spatially with duplicate coordinates. While we want to remove duplicate records, we don't necessarily want to remove *all* duplicates. Our duplicate dataset contains several different species, and sometimes, several species might be in the same location. Rather than removing all but one species in a location, we probably want to remove duplicates *for each species*. This should leave one observation for each species in each location.

To filter our duplicate data by species, we can first split our data by species...

```{r}
#| eval: false
birds |>
  group_split(species)
```

```{r}
#| class: output-scroll
#| echo: false
birds |>
  group_split(species)
```


:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/d/c/4/9/b57dfc5b-06a4-4e98-8836-5239727794cd/original"></img>

::: {.figure-caption}
[*Todiramphus (Todiramphus) sanctus* collecting a crab for lunch. Photo by Peter and Shelly CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/84e11633-bdff-4c37-a759-d1abf2b7d9ba)
:::
:::


...and use `purrr::map()`[^1] to remove duplicates for each species group, binding our dataframes together again with `bind_rows()`.

[^1]: We have used `\(df)` as shorthand within `purrr::map()`. This shorthand can be rewritten as `map(.x = df, function(.x) {})`.<br><br> We provide an input, in this case the piped dataframe which we've called `df`, and use it in a custom function (defined within `{}`). This function is run over each dataframe in our list of dataframes.<br><br>Check out [this description from a recent purrr package update](https://www.tidyverse.org/blog/2022/12/purrr-1-0-0/#documentation) for another example.

```{r}
library(purrr)

birds |>
  group_split(species) |>
  map(\(df) 
      df |> 
        filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
      ) |>
  bind_rows()
```

Splitting by species has reduced the total number of duplicate records by ~3,500 rows because more than one species cmultiple species can have one record with the same coordinates.

## Remove duplicates

To now *remove* these duplicates from our dataframe, we can use the `!` operator to return records that *are not* duplicated, rather than those that are.

```{r}
birds_filtered <- birds |>
  group_split(species) |>
  map(\(df) 
      df |>
        filter(!duplicated(decimalLongitude) & !duplicated(decimalLatitude))) |>
  bind_rows()
birds_filtered
```

To check our results, we can grab a random row from our unfiltered dataframe...

```{r}
test_row <- birds |>
  filter(duplicated(decimalLongitude) & duplicated(decimalLatitude)) |>
  slice(10)

test_row |>
  select(species, decimalLatitude, decimalLongitude, recordID) # show relevant columns
```

...and see whether any rows in `birds_filtered` have the same combination of longitude and latitude coordinates.

```{r}
birds_filtered |>
  filter(
    decimalLatitude %in% test_row$decimalLatitude & 
      decimalLongitude %in% test_row$decimalLongitude
    ) |>
  select(species, decimalLatitude, decimalLongitude, recordID) # show relevant columns

```

As expected, there are a few species with those latitude and longitude coordinates, but we now only have 1 row for each species in that location in `birds_filtered`. 

Using `%in%` can be a powerful tool for finding duplicates in your dataframe. Extracting rows like we did above with our `test_row` example above (or a list of values in a column) can help you weed out more specific duplicate records you are interested in.


Our kingfisher data, `birds_filtered`, is now clean from spatially duplicated records!

```{r}
#| code-fold: true
birds_filtered |>
  rmarkdown::paged_table()
```

:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/d/5/4/b/cec90fd8-8d85-43aa-9d3a-71c3e73db45d/original"></img>

::: {.figure-caption}
[*Ceyx azureus* perched on a log. Photo by andrewpavlov CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/ccba93a8-92e2-4291-ad72-24b9553258f0)
:::
:::

## Summary

This chapter has offered a quick look at how to find duplicate records, remove them from your dataset, and check whether your records have been removed correctly. You may have to check for duplicates in several ways to ensure you have cleaned all duplicate records from your dataset. The next chapter will show you how to clean **missing values** from your dataset.
