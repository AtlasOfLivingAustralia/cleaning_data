---
editor: 
  markdown: 
    wrap: 72
code-annotations: hover
number-depth: 3
---

# Duplicates

Duplicate records can occur for a number of reasons. For instance, a duplicate record might appear in an individual dataset due to errors in data collection or entry, or occur when aggregating multiple data sources. Alternatively, a record might be considered a duplicate in the context of one type of analysis, but not another. For example, prior to running species distribution models, records in the same location---even if they are separate observations---are considered duplicates and should be removed to avoid spatial bias. If you're running multiple models for several time-periods, however, you may need to include records in the same location if they occurred in different time-periods. *Context is key when determining how to identify and clean duplicate records in your dataset.*

Identifying duplicates is important to avoid misleading analyses or visualisations; duplicates can give the impression that there are more data than there really are and bias your analyses to favour certain species, locations, or time periods. In this chapter we will introduce ways of detecting and handling duplicate records in biodiversity data. 

### Prerequisites

In this chapter, we will use kingfisher (*Alcedinidae*) occurrence data in 2023 from the ALA.

```{r prereq-internal}
#| echo: false
#| warning: false
#| message: false
# packages
library(arrow)
library(galah)
library(dplyr)
library(janitor)

galah_config(email = Sys.getenv("ALA_EMAIL"),
             verbose = FALSE)

# get DOIs for chapter
doi_table <- here::here("data", "galah-dois", "doi_table") |>
  arrow::open_dataset() |>
  filter(chapter == "Duplicates") |>
  collect()

# extract dois
doi_birds <- doi_table |> filter(name == "birds") |> pull(doi)

# download
birds <- galah_call() |>
  filter(doi == doi_birds) |>
  atlas_occurrences()
```

```{r prereq-external}
#| echo: false
test <- glue::glue(
"
# packages
library(galah)
library(dplyr)
library(janitor)
galah_config(email = \"your-email-here\") # ALA-registered email

birds <- galah_call() |>
  filter(doi == \"{doi_birds}\") |>
  atlas_occurrences()
"
)
```

```{r prereq-printed}
#| eval: false
#| code: !expr test
#| echo: true
```


:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/2/5/1/d/9525c6da-fd47-41fa-8eff-55c6747ed152/original"></img>

::: {.figure-caption}
[*Todiramphus (Todiramphus) sanctus* perched on a branch. Photo by Kerri-Lee Harris CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/77b8aac0-18af-4ec6-a03c-ff825859a6f3)
:::
:::

<!-- Queries -->
:::{.callout-note collapse="true" appearance="minimal"}

#### Original download queries

**Note:** You don't need to run this code block to read this chapter. It can, however, be useful to see the original download query. This code will download the latest data from the ALA, which you are welcome to use instead, though the data might not exactly reproduce results in this chapter.

```{r prereq-orig-query}
#| eval: false
library(galah)
galah_config(email = "your-email-here")

birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", 
         family, genus, species, cl22, eventDate, month) |>
  atlas_occurrences() # <1>
```
1. We created a custom DOI for our download by using `atlas_occurrences(mint_doi = TRUE)`.

:::

## Find duplicates

As an first example, let's remove all spatially-duplicated records, based on latitude and longitude coordinate values.

The first thing to do is find the duplicate records.

:::{.panel-tabset .nav-pills}
#### dplyr

Return a summary of the number of duplicates for each set of coordinates.

```{r}
birds |> 
  group_by(decimalLongitude, decimalLatitude) |>
  filter(n() > 1) |>
  summarise(n = n(), .groups = "drop")
```

Return a summary of duplicate decimal longitude and latitude rows in the entire dataset. 

```{r}
birds |>
  filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
```

#### janitor

Return duplicated rows and the number of duplicates of `decimalLatitude` OR `decimalLongitude` (note that this differs from the dplyr example because janitor uses commas as an OR statement).

```{r}
birds |> 
  get_dupes(decimalLatitude, decimalLongitude)
```

:::

In the above `tibble` our results show that there are just over 27,000 records that overlap spatially with duplicate coordinates. That seems like a lot! It would be rare to remove duplicates so broadly without considering *why* we need to remove duplicates; we don't necessarily want to remove *all* of them. 

Instead, if we are interested in comparing species in our data, it might be more useful to find duplicate spatial records **for each species**. We can split our data by species and remove records where there is more than one observation of the same species in the same location. This should leave one observation for each species in each location.

To filter our duplicate data by species, we can first split our data by species...

```{r}
#| eval: false
birds |>
  group_split(species)
```

```{r}
#| class: output-scroll
#| echo: false
birds |>
  group_split(species)
```


:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/d/c/4/9/b57dfc5b-06a4-4e98-8836-5239727794cd/original"></img>

::: {.figure-caption}
[*Todiramphus (Todiramphus) sanctus* collecting a crab for lunch. Photo by Peter and Shelly CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/84e11633-bdff-4c37-a759-d1abf2b7d9ba)
:::
:::


...and use `purrr::map()`[^1] to remove duplicates for each species group, binding our dataframes together again with `bind_rows()`.

[^1]: We have used `\(df)` as shorthand within `purrr::map()`. This shorthand can be rewritten as `map(.x = df, function(.x) {})`.<br><br> We provide an input, in this case the piped dataframe which we've called `df`, and use it in a custom function (defined within `{}`). This function is run over each dataframe in our list of dataframes.<br><br>Check out [this description from a recent purrr package update](https://www.tidyverse.org/blog/2022/12/purrr-1-0-0/#documentation) for another example.

```{r}
library(purrr)

birds |>
  group_split(species) |>
  map(\(df) 
      df |> 
        filter(duplicated(decimalLongitude) & duplicated(decimalLatitude))
      ) |>
  bind_rows()
```

Splitting by species has reduced the total number of duplicate records by ~3,500 rows because we've made it possible for multiple species to have records with the same spatial coordinates. 

## Remove duplicates

To now *remove* these duplicates from our dataframe, we can use the `!` operator to return records that *are not* duplicated, rather than those that are.

```{r}
birds_filtered <- birds |>
  group_split(species) |>
  map(\(df) 
      df |>
        filter(!duplicated(decimalLongitude) & !duplicated(decimalLatitude))) |>
  bind_rows()
birds_filtered
```

To check our results, we can grab a random row from our unfiltered dataframe...

```{r}
test_row <- birds |>
  filter(duplicated(decimalLongitude) & duplicated(decimalLatitude)) |>
  slice(10)

test_row |>
  select(species, decimalLatitude, decimalLongitude, recordID) # show relevant columns
```

...and see whether any rows in `birds_filtered` have the same combination of longitude and latitude coordinates.

```{r}
birds_filtered |>
  filter(
    decimalLatitude %in% test_row$decimalLatitude & 
      decimalLongitude %in% test_row$decimalLongitude
    ) |>
  select(species, decimalLatitude, decimalLongitude, recordID) # show relevant columns

```

As expected, there are a few species with those latitude and longitude coordinates, but we now only have 1 row for each species in that location in `birds_filtered`. 

Using `%in%` can be a powerful tool for finding duplicates in your dataframe. Extracting rows like we did above with our `test_row` example above (or a list of values in a column) can help you weed out more specific duplicate records you are interested in.


Our kingfisher data, `birds_filtered`, is now clean from spatially duplicated records!

```{r}
#| code-fold: true
birds_filtered |>
  rmarkdown::paged_table()
```

:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/d/5/4/b/cec90fd8-8d85-43aa-9d3a-71c3e73db45d/original"></img>

::: {.figure-caption}
[*Ceyx azureus* perched on a log. Photo by andrewpavlov CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/ccba93a8-92e2-4291-ad72-24b9553258f0)
:::
:::

## Summary

This chapter has introduced some ways to find duplicated records, remove them from datasets, and check if the changes were correctly made. These methods can be more broadly applied to other types of data as well, not just spatial data. Depending on your analysis, you may need to use bespoke methods for handling duplicates. Later chapters like [Taxonomic validation](../3_cleaning_expert/taxonomic-validation.qmd) and [Geospatial cleaning](../3_cleaning_expert/geospatial-cleaning.qmd) cover more advanced detection and cleaning methods.

In the next chapter, we will discuss ways of handling **missing values** in your dataset. 