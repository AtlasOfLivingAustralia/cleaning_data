---
editor: 
  markdown: 
    wrap: 80
---

# Inspect

Before diving into cleaning, it's always a good idea to familiarise
yourself with the data. In this chapter we will cover ways to **inspect** both the
data and metadata of a dataset.

### Prerequisites

```{r prereq}
#| message: false
#| warning: false
#| echo: false
library(ggplot2)
library(galah)
galah_config(email = Sys.getenv("ALA_EMAIL"),
             verbose = FALSE)
birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", genus, species) |>
  atlas_occurrences()
```

```{r}
#| eval: false
# packages
library(ggplot2)
library(galah)

# data: Kingfisher records from 2023
galah_config(email = "your-email-here") # ALA Registered email

birds <- galah_call() |>
  identify("alcedinidae") |>
  filter(year == 2023) |>
  select(group = "basic", genus, species) |>
  atlas_occurrences()
```

:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/2/5/1/d/9525c6da-fd47-41fa-8eff-55c6747ed152/original"></img>

::: {.figure-caption}
[*Todiramphus (Todiramphus) sanctus* perched on a branch. Photo by Kerri-Lee Harris CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/77b8aac0-18af-4ec6-a03c-ff825859a6f3)
:::
:::

## Getting to know your data

**Metadata** describes your dataset, defines its variables and previews their
contents. Metadata tells you details like what unit of
measurement was used, what the conditions were like, or whether a
certain observation might be an outlier.

Reviewing the metadata of your dataset is always a good first step, as it allows
you to understand the kind of data you are working with and any potential
limitations of the data that might affect your analyses later.

Many datasets include descriptions of the data's taxonomic, spatial and temporal
scope. An example of well formatted metadata is of [FrogID from the Australian
Museum](https://collections.ala.org.au/public/show/dr14760).

![Metadata of the FrogID dataset on the ALA](images/example_frogid.png)

From reading FrogID's metadata [@rowley2020frogid], you'll find:

1.  The data is acoustic data[^1]
2.  This is citizen science data[^2]
3.  Audio is recorded via a smartphone app[^3]
4.  These data record presences, but not absences
5.  The data are under a Creative Commons license which is relevant for reuse and republishing

[^1]: Meaning the majority of the species recorded are male.
[^2]: Suggesting these data could be biased towards populated areas
[^3]: As a result, the authors recommend filtering data to `geographic uncertainty of <3000m` if you require high coordinate precision 


::: {.callout-note collapse="true"}

#### Data standards

Many data infrastructures like the Atlas of Living Australia also follow and encourage a data
standard to help consolidate data from many different data
providers[^inspect-1].

The data standard used by the Atlas of Living Australia is called **Darwin Core**.
Darwin Core works by defining a) a [set of standard terms](https://dwc.tdwg.org/terms/)[^inspect-2] 
to use across datasets as column names, and b) values eligible to be recorded
underneath these terms. Darwin Core standards require that additional files 
detailing metadata and data structure are supplied along with the dataset. This 
helps make sure the data is ingested correctly into the data infrastructure.

Knowing whether your dataset follows a standard can allow you to look up term
definitions to help you familiarise yourself with the data.
:::

[^inspect-1]: Making datasets easier to consolidate is also referred to as
    *interoperability*, [one of the principles of FAIR
    data](https://ardc.edu.au/resource/fair-data/).

[^inspect-2]: We suggest using `Ctrl/CMD + F` and searching your variable name
    on the webpage. Don't hesitate to Google variable names if you are unsure
    what they represent.

## A first glimpse

When starting with a new dataset, we want to get an initial idea:

  * How many rows and columns are there? 
  * What are the column names? 
  * What types of data are in each column? 
  * What are their possible values or ranges? 
  
These answers are useful to know before jumping into wrangling and cleaning data.

There are several ways to return an overview of your data, ranging in how
comprehensively you wish to summarise your data's structure.

::: {.panel-tabset .nav-pills}
#### glimpse()

Return a condensed summary of your data's structure using `glimpse()` from
dplyr.

```{r}
#| message: false
#| warning: false
library(dplyr)

glimpse(birds)
```

#### skim()

Return tables of descriptive statistics for each variable, grouped by data type
(e.g., `numeric`, `character`, `date`) using `skim()` from skimr.

```{r}
#| message: false
#| warning: false
library(skimr)

skim(birds)
```

#### str()

Return a quick summary of your data's structure using base R `str()`

```{r}
str(birds)
```
:::

:::{.aside}

<img class = "rounded" src="https://images.ala.org.au/image/details?imageId=5a0ff06d-d7ac-44d2-a152-b2207cc98ab4"></img>

::: {.figure-caption}
[*Dacelo (Dacelo) novaeguineae* perched with a fresh worm in its beak. Photo by Rob Shepherd CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/1cb231bd-6000-43dd-9feb-409a29e93779)
:::
:::


It is also good at this initial point to evaluate whether your dataset is
what you expect. Ask yourself, does this data look like what you thought it would look like? Are the values in each column reasonable? Are there noticeable gaps or errors that might need
to be fixed (or stop you from using the data altogether)?

## Next steps

We have just learned some ways to initially inspect our dataset. Keep in mind, we don't expect everything in our dataset to be perfect. Some issues are expected, and may reveal that there are issues with our query or with the data themselves. The
initial inspection is a good opportunity to get an idea of where these issues might be and how serious they are.

When you are satisfied that the dataset is largely as expected, you are ready to start summarising your data.

