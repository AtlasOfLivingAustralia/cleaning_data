---
editor_options: 
  chunk_output_type: console
execute:
  cache: true
css: theme.css
---

```{css, echo = FALSE}
.output-green {
  background-color: lightgreen;
  border-radius: 4px;
}
```

```{r}
#| include: false
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(class.output = "output-green")
options(pillar.max_extra_cols = 3)
```

# Dataset integration

When working with biodiversity data it can be common to have data from multiple
sources. Here we will discuss the common obstacles faced when consolidating data
from multiple sources, and present a basic integration workflow of merging two
single species datasets. 

## Setup

<details>
<summary>To see the download process for example data expand here.</summary>

We will download data for the species *Litoria chloris* from the ALA and GBIF
using the `galah` package. For the download request to ALA  the `galah_select()`
is used to return specific fields (see: [Specify fields for occurrence download
â€” galah_select](https://galah.ala.org.au/R/reference/galah_select.html)). This
is a project specific decision and these fields are used as an example.  

```{r}
#| code-fold: true
#| code-summary: Define a character vector of fields to return...
ala_fields <- c(
  "kingdom",
  "taxonRank",
  "phylum",
  "class",
  "order",
  "family",
  "genus",
  "species",
  "taxonRank",
  "countryCode",
  "locality",
  "stateProvince",
  "coordinateUncertaintyInMeters",
  "coordinatePrecision",
  "license",
  "occurrenceID"
)
```

```{r}
#| code-fold: true
#| code-summary: Set `galah_config()` options and download data from ALA...
#| output: false
library("galah")
galah_config(atlas = "Australia") # default
galah_config(email = Sys.getenv("ALA_EMAIL"))
ala_data <- galah_call() |>
  galah_identify("Litoria chloris") |>
  galah_select(basisOfRecord, all_of(ala_fields), group = "basic") |>
  atlas_occurrences()
```

```{r}
#| collapse: true
#| code-fold: true
#| code-summary: Set `galah_config()` options and download data from GBIF...
# For GBIF, a username and password are required
galah_config(
  atlas = "Global",
  username = Sys.getenv("GBIF_USER"),
  email = Sys.getenv("GBIF_EMAIL"),
  password = Sys.getenv("GBIF_PWD")
)

# Search the species name to get the GBIF taxon key
knitr::kable(search_taxa("Litoria chloris"))

# Download data
gbif_data <- galah_call() |>
  galah_identify("2427866", search = FALSE) |>
  atlas_occurrences()
```

</details>

## Inspecting our datasets

The example data we are using (see setup) was downloaded from GBIF and from
Atlas of Living Australia (ALA). Each dataset contains records of a single
species, *Litoria chloris*. 

First, we add a new column to each dataset called source, which acts as an
identifier. This isn't required but its a simple way to keep track of where each
record came from after the data has been merged. 

```{r}
# add identifier column to each dataset
ala_data$source <- "ALA"
gbif_data$source <- "GBIF"
```

To merge datasets, we need to understand what variables are in each dataset, and
how they are formatted. 

- How many columns does each dataset have?
- What are the column names?
- Do the column names match between the two datasets?
- What are the data types of each column?

```{r}
#| collapse: true
#| code-fold: true
#| code-summary: Take a glimpse at the data
dplyr::glimpse(ala_data)
dplyr::glimpse(gbif_data)
```

```{r}
length(colnames(ala_data))
length(colnames(gbif_data))
```

Comparing the column names is important because the function we will use merges
data based on shared column names. Therefore, we want any shared variables
across datasets, such as scentific name or decimal coordinates, to also have the
same column name in both dataframes. If a column containing latitude values is
named `Latitude` in one dataset and `latitude` in the other, the final dataset
will have two columns, instead of one single latitude column. 

Here we will show how to do a quick inspection using the base `intersect()` and
`setdiff()` functions, but there are some great packages available to conduct a
more complete comparison, such as the waldo package, or the Arsenal package's
`comparedf()` function.

```{r}
# waldo::compare(ala_data, gbif_data, max_diffs = 100)

# Quick comparison
common_cols <- intersect(names(ala_data), names(gbif_data))

# Find columns unique to each dataset
unique_ala_data <- setdiff(names(ala_data), names(gbif_data))
unique_gbif_data <- setdiff(names(gbif_data), names(ala_data))

# Summary
sprintf("Number of common columns: %i", length(common_cols))
sprintf("Common column names: %s", paste(common_cols, collapse = ", "))
sprintf("Columns unique to ala_data: %s", unique_ala_data)
```

## Selecting and renaming columns

Subset each dataset, selecting only the columns or variables that you want to
have in the final dataset. Merging a subset of only the needed columns can
reduce clutter in the merged dataset. 

This is just an example of selecting columns; the columns you select will depend
on your requirements:

```{r}
#| eval: false
columns_keep_data_A <- c("column1", "column2", "column3", "column4", "column5")
data_A_subset <- dplyr::select(data_A, all_of(columns_keep_data_A))
```

Repeat for the other dataset. Remember, your column names may differ between
datasets, so you should refer to your comparisons to determine which name to use
when selecting for each dataset, and then rename those columns to match the
first dataset. The order of operations here is subset -> conform names -> merge. 

```{r}
#| eval: false
columns_keep_data_B <- c(
  "column_a", "column_s", "column_d", "column_f", "column_g"
)
data_B_subset <- dplyr::select(data_B, all_of(columns_keep_data_B))
data_B_subset_renamed <- dyplr::rename(data_B_subset,
  column1 = column_a,
  column2 = column_s,
  column3 = column_d,
  column4 = column_f,
  column5 = column_g
)
```

::: {.callout-note}
Note, instead of renaming columns to match a particular dataset, you can of
course pick your own column naming scheme, and rename the other datasets to match.
:::

## Merging datasets

We can merge the datasets using `dplyr::bind_rows()`. This function binds data
frames on top of each other, by matching columns by name. Dataframes that were
missing columns present in another dataframe will have `NA` values for that
column. 

```{r}
merged_data <- dplyr::bind_rows(ala_data, gbif_data)
write.csv(merged_data, "data/galah/chloris.csv")
```

::: {.callout-caution}
The `bind_rows()` function requires columns of the same name to have the same
data type. If you get an error, check the data types of the flagged columns in
each of your datasets and conform them to the same data type.
:::

After mering datasets, you may see many rows of NA values, depending on whether
those columns were shared or not. This is a good time to look through the merged
dataset and check if the merge went as expected. You can start by using the
inspection code or packages mentioned above. Note that some of these steps are
also cleaning steps that will be repeated later in detail. The focus here is
primarily to identify any obvious merge issues that can be resolved before
moving on. 
 
Some things to check are:
- Are there any columns missing that should be present?
- Are there any columns that should be present but are missing?
- Are there any unexpected columns that you didn't intend to include?
- Are there any unexpected values, such as NA values in columns that you
expected to be shared and have values for all records?

```{r}
# Check for missing values
library("dplyr")

merged_data %>%
  dplyr::group_by(source) %>%
  dplyr::summarise(across(everything(), ~ sum(is.na(.)),
    .names =
      "na_in_{.col}"
  )) %>%
  # reshape for easier comparison
  tidyr::pivot_longer(
    cols = -source, names_to = "column", names_prefix =
      "source_"
  ) %>%
  tidyr::pivot_wider(names_from = source, values_from = value) %>%
  # add percentage #TODO
  dplyr::mutate(percentage = round(100 * (GBIF + ALA) / nrow(merged_data), 2)) %>%
  dplyr::arrange(desc(percentage)) %>%
  dplyr::filter(percentage < 55 & percentage > 45) %>%
  head(10)
```

This is one approach that can help - looking at columns ~50% missing to track
down potential merge issues (expected a column to be shared and merged but it is
missing values in the merged dataset). This method is also generally useful for
detecting missing values in columns, which is discussed later in the book. 

## Summary

We have briefly introduced how to merge two disparate datasets, including a few
data checking producedures relevant to merging data. But it should be noted that
some of these checks are generally useful during the data cleaning process, and will appear throughout the book. 

## Advanced technique

Performing checks and operations on more than one dataset using separate objects
like we have shown above is a good way to get started. A more efficient option
can be to use a list of datasets and perform operations on the list, particuarly
when integrating many datasets. Using a list reduces the potential for mistakes
that can occur when duplicating code. Another option would be a for loop, or the
`purrr` package.

```{r}
#| eval: false

# Examples on how to get started with a list of datasets:
datasets <- list(
  ala = ala_data,
  gbif = gbif_data
)
# Check the number of records in each dataset
lapply(datasets, nrow)
# Check the number of columns in each dataset
lapply(datasets, ncol)
# Check the column names in each dataset
lapply(datasets, colnames)
# With apply functions we can operate over list elements with custom functions
datasets_source <- lapply(names(datasets), function(name) {
  datasets[[name]]$source <- name
  datasets[[name]]
})

```

## Alternative method: spocc package

Using `spocc` package to download data from GBIF and ALA with a combined query:

```{r}
library(spocc)
out <- occ(
  query = "Litoria chloris",
  from = c("gbif", "ala"),
  gbifopts = list(hasCoordinate = TRUE), limit = 10
)
head(out$gbif$data[[1]], 3) # / out$ala$data
```

The `occ2df()` function converts the results into a single data frame. Note:
this function doesn't handle duplicate observations, you still need to check for
duplicates and decide how to handle them (see duplicates section #TODO link).

```{r}
out_data <- occ2df(out, what = "data")
head(out_data, 3)
```

This is a fast way to get started, but it doesn't include all returned data
columns. If we need those columns, we can manually bind the data from each source.

```{r}
occ_ala <- occ2df(out$ala)
occ_gbif <- occ2df(out$gbif)
# bind_occ <- dplyr::bind_rows(occ_ala, occ_gbif)
```

Here we get a helpful error. The `bind_rows` function requires columns of the
same name to have the same data type, and the error message tells us that month
is a character in one dataset and an integer in the other. We can solve this by
converting one of the columns to match, and try again. 

```{r}
typeof(occ_ala$recordedBy)
occ_ala$month <- as.integer(occ_ala$month)

# recordedBy column is also different so we will fix this as well
typeof(occ_ala$recordedBy)
occ_ala$recordedBy <- as.character(occ_ala$recordedBy)

# Now we can successfully bind the data
bind_occ <- dplyr::bind_rows(occ_ala, occ_gbif)
```

Now we inspect the `bind_occ` data set for any issues (using techniques
discussed above). Some examples:

The field `class` has a typo in one dataset, the result is that
our data contains one column `classs` and one `class`. We can fix this with the
`coalesce()` function from `dplyr`. This finds the first non-missing value at
each position. We can then remove `classs`.

```{r}
bind_occ$class <- dplyr::coalesce(bind_occ$class, bind_occ$classs)
bind_occ <- dplyr::select(bind_occ, -classs)
```

There are NA values for scientificName from the ALA. But we also have column
`species` with values for both datasets, so this is not an issue here, we can
just use `species`. 

Looking at the species column, there are two different names. This is because
the ALA taxonomic backbone for fauna is the Australian Faunal Directory (AFD),
while GBIF uses a taxonomic backbone of their own. In this case, GBIF recognised
our query for *Litoria chloris* as a synonym of *Ranoidea chloris*. ^[If you're
interested, phylogenetic papers in recent years proposed taxonomic changes to
Hylidae. However these changes have not been adopted by the AFD at this point in
time, preferring to maintain the traditional and imperfect Litoria pending a
detailed revision of Australian hylids that is in progress.] We will talk more
about taxonomy issues like this and how to approach them later in the book.