---
number-depth: 3
editor: 
  markdown: 
    wrap: 80
code-annotations: hover
---

# Where to get data

There are *many* types of biodiversity data to work with. Some examples include
observational data of where a species has been observed, climate or
environmental data for region or area, biological data to compare measures of
organisms' physical or behavioural traits, and genetic data to compare unique
DNA or alleles between individuals.

Here, we'll detail what open source data is, provide some places to look for
open source data and suggest some R packages that are useful for downloading
different types of data.

### Prerequisites

First, we'll load packages that we'll need to display data and figures over the
chapter.

```{r}
#| warning: false
#| message: false
library(sf)
library(ggplot2)
library(dplyr)
library(tidyterra)
library(terra)
library(here)
```

## Open-source data

Open-source data are data made openly accessible for editing and use, licensed
under [an open license](https://creativecommons.org/about/open-data/). The
following are some places where you can find open-source data.

### Biodiversity data

::: {.panel-tabset .nav-pills}
#### Living Atlases

Living Atlases are national or regional infrastructures that aggregate
biodiversity data from many different sources. These sources include research
projects, government monitoring programs, museums & herbaria, and citizen
science apps like iNaturalist and eBird. Some examples are:

| Country/Region | Name                                                                                                 | Acronym |
|---------------------|--------------------------------------|---------------------|
| Australia      | [Atlas of Living Australia](https://www.ala.org.au)                                                  | ALA     |
| Austria        | [Biodiversitäts-Atlas Österreich](https://biodiversityatlas.at)                                      | BAO     |
| Brazil         | [Sistema de Informação sobre a Biodiversidade Brasileira](https://www.sibbr.gov.br/)                 | SiBBr   |
| Estonia        | [eElurikkus](https://elurikkus.ee)                                                                   |         |
| France         | [Portail français d'accès aux données d'observation sur les espèces](https://openobs.mnhn.fr)        | OpenObs |
| Guatemala      | [Sistema Nacional de Información sobre Diversidad Biológica de Guatemala](https://snib.conap.gob.gt) | SNIBgt  |
| Portugal       | [GBIF Portugal](https://www.gbif.pt)                                                                 | GBIF.pt |
| Spain          | [GBIF Spain](https://www.gbif.es)                                                                    | GBIF.es |
| Sweden         | [Swedish Biodiversity Data Infrastructure](https://biodiversitydata.se)                              | SBDI    |
| United Kingdom | [National Biodiversity Network](https://nbn.org.uk)                                                  | NBN     |

Living Atlases work with local data providers to ingest and standardise
open-source data, and some even use a specific taxonomic backbone. If your
project is focused on a specific region, downloading data directly from a
regional node may be more appropriate.

[See a complete list of existing national and regional Living
Atlases](https://www.gbif.org/the-gbif-network).

#### GBIF

The [Global Biodiversity Information Facility (GBIF)](#0) is an international
data infrastructure and a network that aggregates biodiversity data from the
many Living Atlases around the world. GBIF acts as an overarching organisation
to store and provide these data from the Living Atlas "nodes" using a unified
data standard.

At present, GBIF manages and serves over 2.6 billion occurrence data points!

[See a complete list of national and regional nodes that contribute to
GBIF](https://www.gbif.org/the-gbif-network).

#### Biocollections

Biocollections are data infrastructures that hold specimen data from museums and
collections.

Some examples include:

| Name                                                             | Description                                                                                         |
|-------------------------------|-------------------------------------------------|
| [Integrated Digitzed Biocollections](https://www.idigbio.org)    | Holds data of biological specimens that have been made electronically available (i.e., *digitised*) |
| [VertNet](https://www.vertnet.org/)                              | Holds data of vertebrate specimens from more than 400 collections & 120 publishers                  |
| [Australasian Virtual Herbarium (AVH)](https://avh.chah.org.au/) | Holds over eight million plant, algae and fungi specimens                                           |

#### Data providers

If your project relates to data from a specific data provider, it also might be
best to download data directly from the source.

For example, a common citizen science tool to collect species observations is
[iNaturalist](https://www.inaturalist.org/). Downloading directly from the
original data source can help to ensure you don't have any stray data from other
sources in your download. You can directly contact data providers to ensure the
data hasn't been pre-filtered before downloading.
:::

```{=html}
<!-- 
I feel like this is an important point to make in this book, but it is kinda hidden here. It could go in the intro, but there's not a particular place for this kind of info there either? - DK 
-->
```
<!-- Long-form version -->

::: {.callout-note collapse="true"}
#### Why do we still have to clean data from data infrastructures?

Data infrastructures like the Atlas of Living Australia ingest, aggregate and
standardise millions of rows of data from thousands of data providers. Some data
comes from large providers with standardised workflows, like state government
monitoring programs, [iNaturalist Australia](https://inaturalist.ala.org.au/) or
[eBird](https://ebird.org/home). These data providers use workflows that attempt
to remove suspicious records prior to sharing data with a Living Atlas, and, in
general, these workflows catch many issues that otherwise might need fixing.

However, not all data providers have standardised workflows. Some data has been
transcribed from written survey records and provided by a small or independent
data provider. Other data might have been transcribed from archived written
records in a museum, or even in a scientists backlog from a long-lost research
project. These data are valuable but inevitably prone to errors that are
difficult to fix---handwriting can be smudged or difficult to read, records
might be lacking important details about their location or time of observation.
Even in data from standardised workflows, errors like taxonomic
misidentification or flipped geospatial coordinates can slip through the cracks
because expert knowledge is required to identify and amend individual records.
These records can also range in their precision or level of detail, and might
not be suitable for every type of analysis.

Ultimately, it's a team effort to remove or fix data issues. Although a data
infrastructure can use programmatic data quality checks to try to remove more
extreme outliers, many errors are context dependent and require verification
from the original data provider. This means that the responsibility to fix
records usually falls on the data provider because only the data provider has
knowledge required to amend their original data. Inevitably, there will be
errors in data from many different sources, and equipped with this knowledge, we
still need to clean data from data infrastructures to be suitable for our
research question or analysis.
:::

<!-- This could be a shorter version of the above? -->

```{=html}
<!--
::: {.callout-note collapse="true"}
#### Why do we still have to clean data held in data infrastructures? (short)

Data infrastructures aggregate data from thousands of different data
providers, and standardise them so that data from many disparate sources
can be used together.

Data providers, however, can provide data with mistakes, and data
infrastructures are not overarching taxonomic or ecological experts.
Ultimately, it's a team effort to catch errors, and it's up to data
providers to double check that any flagged errors are indeed actually
errors.
:::
-->
```
### Spatial data

Spatial data contain information that corresponds to an area on the globe and
can be plotted onto a map. Spatial data can be represented as vector or raster
data.

There are two types of spatial data that you will probably use:

::: {.panel-tabset .nav-pills}
#### Vectors

**Vectors** are data for drawing points, lines and shapes. They contain a
`geometry` column which contains information to draw points, lines or polygons
onto a map.

Country or region outlines are often saved as vectors, which are typically
loaded using an R package like
{[rnaturalearth](https://github.com/ropensci/rnaturalearth)} or by reading in a
`.shp` file using the {sf} package.

```{r}
#| eval: false
state_outline <- sf::st_read("path/to/file.shp")
```

Here is an example of what vector data looks like in R...

```{r}
ozmaps::ozmap_states
```

...and what it looks like when plotted with ggplot2.

```{r}
ggplot() + 
  geom_sf(data = ozmaps::ozmap_states) + 
  theme_void()
```

#### Rasters

**Rasters** are data for drawing a layer of data values over a gridded area.
They contain values of a variable (like temperature) for each pixel of a grid,
and Each pixel of the grid represents a square area (e.g., 1 km^2^). Just like
how the smaller each pixel is on a TV screen the higher its definition, the
smaller each square is in a raster layer the higher its resolution.

Climate data is often saved as a raster, which is typically loaded using an R
package like {[geodata](https://github.com/rspatial/geodata)} or by reading in a
`.tif` file using the {terra} package.

<!-- Download this file from Teams /data/data_cleaning_book/ -->

```{r}
world_clim_raster <- rast(here("data", "rasters", "aggregated_bioclim.tif"))
```

Here is an example of what raster data looks like in R...

```{r}
world_clim_raster
```

...and what it looks like when plotted with tidyterra and ggplot2. This map
displays Australia's annual mean temperature (BioClim 1) in low-resolution.

```{r}
ggplot() +
  geom_spatraster(data = world_clim_raster,
                  mapping = aes(fill = wc2.1_30s_bio_1)) +
  theme_void()
```
:::

Here are some examples of where to download spatial data.

::: {.panel-tabset .nav-pills}
#### Climate data

[WorldClim](https://www.worldclim.org/data/index.html) is a database of global
gridded climate and weather data for historic, current and future conditions.

Ecologists and biologists tend to work specifically with [Bioclimatic
variables](https://www.worldclim.org/data/bioclim.html) (BioClim). which are
typically more meaningful variables for understanding biological things, derived
from fluctuations in temperature and rainfall.

Examples of BioClim variables include Temperature Annual Range, Annual
Precitipation, or Precipitation in the Wettest or Driest month. [See the
complete list of BioClim
variables](https://www.worldclim.org/data/bioclim.html).

Rasters are read into R as a `.tif` file.

#### Shapefiles

Shapefiles are vector data with information to draw the outline of one or more
specific areas or regions.

One of the best ways to search for shapefiles is Google. Some of the safest
places to find up-to-date shapefiles are on national or regional government
websites. For example, the [Australian Bureau of Statistics (ABS)]() holds
shapefiles with many levels of regional boundaries, ranging from
states/territories to local government areas.

Shapefiles are read into R as a `.shp` file. These `.shp` files are usually
within a folder (often a zipped folder) that contains several other files that
help to build the `.shp` file when it is loaded. Here is an example of the
contents of an unzipped folder containing a shapefile:

![A folder containing the shapefile for local government
areas](images/example_shapefile-folder.png)

They are then read into R using a function like `st_read()` from the sf package.

```{r}
#| eval: false
library(sf)
library(rmapshaper)

shapefile <- st_read(here("path",
                          "to",
                          "shapefile.shp"),
                     quiet = TRUE) |>
  ms_simplify(keep = 0.1) # <1>
```

1.  Many shapefiles are a large file size. `ms_simplify()` from the rmapshaper
    package simplifies the shapefile by reducing the number of points that make
    up the polygon while maintaining its overall shape. This is a handy way to
    reduce the size of your shapefile in R.
:::

### Taxonomic data

Taxonomy is a complex and broad field of investigation. A comprehensive look
into taxonomy is well outside the scope of this book. However, It's a good idea
to consider the taxonomic classification of the organism(s) you're interested in
and any potential naming differences between data sources.

We do advise that before deciding on a final taxonomy to download or use, it's
worth being aware of what naming authority your data is using as its taxonomic
backbone. In some taxonomic groups, names can vary widely depending on what
taxonomic authority is used. Double check your data after your download them to
make sure the classifications you expect are what you finding. This check will
help prevent errors later on (though you might still need to re-code data
manually).

We discuss these considerations in more detail in the [Taxonomic Validation
chapter](../3_cleaning-expert/taxonomic_validation.qmd).

Here are some examples of where to find Australian taxonomic information.

| Name                                                                          | Description                                                                                    |
|------------------------------------|--------------------------------------------|
| [The Australian Faunal Directory (AFD)](https://biodiversity.org.au/afd/home) | An online catologue of nomenclature and taxonomy of animal species known to occur in Australia |
| [The Australian Plant Name Index (APNI)](https://www.anbg.gov.au/apni/)       | A tool for the botanical community containing accepted scientific names of plants              |
| [The Australian Plant Census]()                                               | Contains the currently accepted scientific names for Australian vascular flora.                |

### Trait data

Trait data contains measurements of organisms' morphological or behavioural
traits (e.g., stem length, leaf size, egg size, migratory distance, soil
carbon). These data are useful for comparing spatial or temporal differences
between individuals, groups or species.

The following are some examples of where to find trait data.

| Name          | Description                                                                                                                                                                                                        |
|--------------------|------------------------------------------------------------|
| [Austraits]() | A plant trait database that synthesises data from field surveys, published literature, taxonomic monographs, and individual taxon descriptions. The database holds nearly 500 traits across more than 30,000 taxa. |

: {tbl-colwidths="\[25,75\]"}

## Packages for downloading data

There are a range of R packages available for accessing biodiversity data. These
packages serve as convenient interfaces to various data providers by making
respective APIs usable directly within R. The functionality offered by these
packages typically ranges from querying species occurrence records, to more
comprehensive taxonomic and spatial download queries.

Below, we highlight some commonly used packages. We encourage users to explore
the documentation of each package to understand their capabilities, which will
help you select one (or more!) that align with your specific needs.

### Occurrence data

#### galah

[galah](https://galah.ala.org.au/) is an interface for accessing biodiversity
data like occurrences, counts, species and media (e.g., images & sounds) from
the Living Atlases and GBIF.

In the majority of examples over this book we will be using the `galah` package.
One benefit of using `galah` is that it uses tidy syntax (much like dplyr) to
edit & filter download queries. Additionally, galah can access data from 10
other Living Atlases and GBIF.

```{r}
#| warning: false
#| message: false
#| eval: false
library(galah)

galah_config(email = "your-email-here") # Registered ALA email

galah_call() |>
  identify("perameles") |>
  filter(year == 2001) |>
  atlas_occurrences()
```

```{r}
#| warning: false
#| message: false
#| echo: false
library(galah)

galah_config(email = Sys.getenv("ALA_EMAIL"), 
             verbose = FALSE) # Registered ALA email

galah_call() |>
  identify("perameles") |>
  filter(year == 2001) |>
  atlas_occurrences()
```

#### Other packages

::: {.panel-tabset .nav-pills}
#### rgbif

[rgbif](https://docs.ropensci.org/rgbif/) searches and retrieves data from the
Global Biodiversity Information Facility (GBIF).

```{r}
#| eval: false
library(rgbif)

# Download occurrences
occ_search(scientificName = "Calopteryx splendens",
           country = "DK",
           year="1999,2005")
```

#### rinat

[rinat](https://docs.ropensci.org/rinat/) is an R wrapper for accessing
[iNaturalist](https://www.inaturalist.org) observations.

```{r}
#| eval: false
library(rinat)

# Download occurrences
get_inat_obs(taxon_name = "Colibri",
             quality = "research",
             maxresults = 500)
```

#### rebird

[rebird](https://docs.ropensci.org/rebird/) provides access to the
[eBird](https://ebird.org/home) web services.

```{r}
#| eval: false
library(rebird)

# Download occurrences
ebirdgeo(species = species_code('spinus tristis'), 
         lat = 42, 
         lng = -76)
```

#### spocc

[spocc](https://docs.ropensci.org/spocc/) queries and collects species
occurrence data from a variety of sources, including
[GBIF](https://www.gbif.org/), the [ALA](https://ala.org.au/),
[iDigBio](http://www.idigbio.org/) and
[VertNet](https://github.com/ropensci/rvertnet). spocc is particularly useful
because it allows for a single download request in R to query and return data
from multiple data sources in a single nested dataframe.

```{r}
#| eval: false
library(spocc)

# Download occurrences
df <- occ(query = 'Accipiter striatus', 
          from = c('gbif', 'idigbio'), 
          limit = 25)
occ2df(df)
```
:::

### Spatial data

::: {.panel-tabset .nav-pills}
#### geodata

[geodata](https://github.com/rspatial/geodata) contains data of climate,
elevation, soil, crop, species occurrence and administrative boundaries.

```{r}
#| eval: false
# Download world climate data
worldclim <- worldclim_country(
    country = "Australia",
    var = "bio",
    res = 5,
    path = here::here("path", "to", "folder")
  )
```

#### ozmaps

[ozmaps](https://github.com/mdsumner/ozmaps) contains *simple features* (`sf`)
data for plotting maps of Australia and its regions.

```{r}
#| eval: false
library(ozmaps)

aus <- ozmap_data(data = "states")

ggplot() +
  geom_sf(data = aus) + 
  theme_void()
```

#### rnaturalearth

[rnaturalearth](https://github.com/ropensci/rnaturalearth) contains *simple
features* (`sf`) data for plotting world maps, countries, sovereign states and
map units.

```{r}
#| eval: false
library(rnaturalearth)

# Download outline of Brazil
brazil <- ne_countries(scale = "medium", 
                       continent = 'south america', 
                       returnclass = "sf") |>
  filter(name == "Brazil")

ggplot() +
  geom_sf(data = brazil) +
  theme_void()
```

#### elevatr

[elevatr](https://github.com/jhollist/elevatr) downloads elevation data from
various sources like AWS Open Data Terrain Tiles.

```{r}
#| eval: false
library(elevatr)
library(rnaturalearth)

# Download outline of Cambodia
cambodia <- ne_countries(scale = "medium", 
                         continent = 'asia', 
                         returnclass = "sf") |>
  filter(name == "Cambodia")

# Download elevation data for Cambodia
cambodia_elev <- get_elev_raster(locations = cambodia, 
                                 z = 11, 
                                 clip = "locations", 
                                 neg_to_na = "TRUE")
```
:::

### Trait data

::: {.panel-tabset .nav-pills}
#### austraits

[austraits](https://traitecoevo.github.io/austraits/) allows users to access,
explore and wrangle plant trait data from the [AusTraits
database](https://austraits.org/), which synthesises 500 traits across more than
30,000 taxa.

```{r}
#| eval: false
library(austraits)

# load database
austraits <- load_austraits(version = "4.0.1", 
                            path = "path/to/folder")

# extract data by trait
wood_density <- austraits |> 
  extract_trait("wood_density")
```
:::

## Summary

Over this chapter, we hope you have found some ideas of where to access
biodiversity data. The following chapter will help explain how to work with
large datasets in R.
