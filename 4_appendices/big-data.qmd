---
editor: 
  markdown: 
    wrap: 72
code-annotations: hover
number-depth: 3
---

# Big data

Once the size of your dataset starts to approach the size of your computer's memory (or vastly exceeds it!), it becomes difficult to process this data using R. There are a few different approaches to managing this, such as using the [arrow package](https://arrow.apache.org/docs/r/index.html) for larger than memory data workflows or the [duckdb package](https://r.duckdb.org/index.html) for accessing and managing databases.  

Here we briefly summarise how arrow and duckdb work. For more complete summaries of these packages, check out the [arrow chapter](https://r4ds.hadley.nz/arrow) and [databases chapter](https://r4ds.hadley.nz/databases) in *R for Data Science*.

### Prerequisites

In this chapter, we will use *Pardalote* occurrence data. You will need to save the `pardalotes` data locally (i.e. write to disk) as a csv file to use over this chapter.

```{r prereq-internal}
#| echo: false
#| warning: false
#| message: false
# packages
library(here)
library(dplyr)
library(readr)

# get DOIs for chapter
doi_table <- here::here("data", "galah-dois", "doi_table") |>
  arrow::open_dataset() |>
  filter(chapter == "Big data") |>
  collect()

# extract dois
doi_pardalotes <- doi_table |> filter(name == "pardalotes") |> pull(doi)
```

```{r prereq-external}
#| echo: false
test <- glue::glue(
"
# packages
library(galah)
library(here)
library(readr)
galah_config(email = \"your-email-here\") # ALA-registered email

galah_call() |>
  filter(doi == \"{doi_pardalotes}\") |>
  atlas_occurrences() |>
  write_csv(here(\"data\", \"pardalotes.csv\"))
"
)
```

```{r prereq-printed}
#| eval: false
#| code: !expr test
#| echo: true
```

:::{.aside}

<img class = "rounded" src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/7/6/2/0/1b7b3e7a-80d8-49e2-be69-f2b7ebb10267/original"></img>

::: {.figure-caption}
[*Pardalotus (Pardalotus) punctatus* eating a snack. Photo by sirkendizzle CC-BY-NC 4.0
(Int)](https://biocache.ala.org.au/occurrences/90dfaafd-a77c-4ccc-842d-3218a1ce64d2)
:::
:::

<!-- Queries -->
:::{.callout-note collapse="true" appearance="minimal"}

#### Original download queries

**Note:** You don't need to run this code block to read this chapter. It can, however, be useful to see the original download query. This code will download the latest data from the ALA, which you are welcome to use instead, though the data might not exactly reproduce results in this chapter.

```{r prereq-orig-query}
#| eval: false
library(galah)
galah_config(email = "your-email-here")

pardalotes <- galah_call() |>
  identify("Pardalotus") |>
  filter(year >= 2015) |>
  select(genus, 
         species, 
         scientificName, 
         cl22,
         year,
         month, 
         decimalLatitude,
         decimalLongitude) |> 
  atlas_occurrences() # <1>
```
1. We created a custom DOI for our download by using `atlas_occurrences(mint_doi = TRUE)`.

:::

### arrow  

The [arrow package](https://arrow.apache.org/docs/r/index.html) allows users to analyse larger-than-memory datasets using dplyr verbs. A common workflow might consists of reading in some data, filtering or summarising according to some property of the data, and writing this to a new file. This is relatively simple to do with a small csv file, and we can do something conceptually similar using arrow for larger datasets[^large-dataset]. 

[^large-dataset]: The dataset used in this example is relatively small (~ half a million rows), but the benefits of using arrow become obvious once the number of rows in your dataset approaches tens or hundreds of millions.

We can open the csv file we just downloaded as a dataset instead of reading it into memory...
 
```{r}
#| message: false
#| warning: false
library(arrow)
pardalotes <- open_dataset(here("data", "pardalotes.csv"), format = "csv")
glimpse(pardalotes)
```

...and perform some common operations on it before finally bringing it into memory with `collect()`. 

```{r}
#| message: false
#| warning: false
pardalotes |> 
  select(species, year, cl22) |> 
  filter(species != "NA", cl22 != "NA")|> 
  group_by(species, year, cl22) |> 
  summarise(count = n(), .groups = "drop") |>
  collect()
```

arrow has many other functions that make working with big data as smooth as possible, such as reading and writing different file formats (including **parquet**[^parquet] and **feather** for efficient storage), partitioning datasets for more effective querying, working with multi-file datasets, and interacting with cloud storage. 

[^parquet]: Parquet files use a columnar storage format (rather than a row storage format, as csv files do) and provide an especially efficient way to store and retrieve large datasets. 


### duckdb and dbplyr  

[DuckDB](https://duckdb.org/) is a database management system that can read, write, and manipulate larger-than-memory datasets using [SQL](https://aws.amazon.com/what-is/sql/) (a standard language for accessing and manipulating databases). This can be very useful for working with large relational tables, for instance, or appending large amounts of data to existing datasets. To do access and manipulate these datasets programmatically in R without having to use SQL, we can use [dbplyr](https://dbplyr.tidyverse.org/) together with [duckdb](https://r.duckdb.org/index.html). 

As a small example, we'll perform a similar operation to the one we did above using arrow. Here, we write the `pardalotes` dataset to an in-memory database and then summarise it. 

```{r}
#| warning: false
#| message: false
library(duckdb)
library(dbplyr)  

con <- dbConnect(duckdb()) 
duckdb_register(con, "pardalotes", pardalotes)  
  
dplyr::tbl(con, "pardalotes") |>
  select(species, year, cl22) |> 
  filter(species != "NA", cl22 != "NA")|> 
  group_by(species, year, cl22) |> 
  summarise(count = n(), .groups = "drop") |>
  collect()
  
dbDisconnect(con)  
```

One of the really cool things about duckdb and arrow is **zero-copy integration**, which allows you to pass datasets back and forth between the two engines within an analysis pipeline with very little loss of efficiency. If there is some functionality that is not supported by one of the packages but is supported by the other, you can simply switch in the middle of an analysis! 

The duckdb and arrow blogs have written about this in greater detail [here](https://arrow.apache.org/blog/2021/12/03/arrow-duckdb/) and [here](https://duckdb.org/2021/12/03/duck-arrow.html). 

