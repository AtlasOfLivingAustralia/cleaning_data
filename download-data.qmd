---
bibliography: references.bib
---

# Download occurrence data {#sec-download-data} 

Once you have decided on your data scope, we can precede to downloading the data. We will introduce a few common data infrastructures that offer open access biodiversity data and highlight some considerations when choosing one in the context of your data scope. We will then discuss some obstacles when consolidating data from multiple sources and the importance of metadata. 

## Where to get data from

The [Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/) network consists of a series of ‘node’ organisations who collate biodiversity data from their own countries, with GBIF acting as an overarching organisation to store data from all nodes. Users that are interested in obtaining data that has global coverage may want to download directly from GBIF. 

Alternatively, using a *regional node* may be more relevant if your project is at a smaller scale. For example, the [Atlas of Living Australia (ALA)](www.ala.org.au) is the Australian node to GBIF and aggregates data from a broad range of providers such as government initiatives, museums, and universities. Importantly, the ALA uses their own taxonomic system and may vary with other data infrastructures. To find other nodes, check out this [page](https://www.gbif.org/the-gbif-network)

If your project relates to citizen science then [iNaturalist](https://www.inaturalist.org/) may be a good option for accessing crowd-sourced data of species observations.


## Downloading data

Many developers have created R packages to interact with each data infrastructures's API to aid access to biodiversity data. Here are a few examples, we recommend taking a look at each package's documentation to choose one that suits your project. Below we have included some code blocks for downloading occurrence data. 

- [`rgbif`](https://docs.ropensci.org/rgbif/) an interface to GBIF
- [`galah`](https://galah.ala.org.au/index.html)  an interface to a number living Atlases, as well as GBIF
- [`rinat`](https://docs.ropensci.org/rinat/) an interface to iNaturalist observations
- [`rebird`](https://docs.ropensci.org/rebird/) an interface with the eBird webservices.
- [`spocc`](https://docs.ropensci.org/spocc/) a R package to query and collect species occurrence data from various sources including [VertNet](https://github.com/ropensci/rvertnet), [iDigBio](http://www.idigbio.org/) and others.

One benefit of using `galah` is that enables users to acquire not only species occurrence records but also taxonomic information, or associated media such as images or sounds.

```{r}
## example for downloading data (ALA and maybe also GBIF)
```

### Refining your data download

It is typical that data infrastructures impose constraints to query size. We suggest for broad data scopes that it may be more efficient to further refine your query before downloading. Below we provide two 

#### Basis of record

When handling data from the ALA, some filters can help obtain only the type of records required for the study. Filter by "basis of record", for example, can ensure consistency when getting data from multiple places [@fuhrding-potschkat_influence_2022]. For example, it is possible to exclude data originating from citizen science projects or only include data from herbariums [@godfree_implications_2021] and natural history museums. This can help indicate where the data has come from, and it's quality. If data has been sourced from multiple places this can assist with making the data merging step more streamlined.

```{r}

```

#### Year filter

- **WHEN** - from what time period do you want your data to cover?

Many people when working with open source biodiversity data will remove all records before a certain year [@gueta_quantifying_2016; @marsh_accounting_2022]. This is often done to increase data precision, as older records are more likely to be insufficiently accurate. The year cutoff varies however the most commonly used is 1945 [@zizka_no_2020; @fuhrding-potschkat_influence_2022], although some studies discard all data collected before 1990 [@gueta_quantifying_2016; @marsh_accounting_2022].

## Merge datasets

If you have downloaded data from different sources, you likely will need to collate your data into a singular database.

When combining data from multiple places, it is important to standardize the data fields and merge the data carefully [@ribeiro_bdc_2022]. There is a chance data will be incorrectly formatted and/or mislabeled.

One of the main issues you might face if you've sourced data from different organisations/people is that higher taxonomy may not match- if this is the case, take a look at the "taxonomy" chapter for more information on how to deal with inconsistent taxonomy before merging!


```{r}
##example on formatting consistency, merging data together etc
```

## Familiarize with the data and metadata

Now that you have data to work with it's important to familiarise yourself with it. This is best done before you get started with any data cleaning [@streamdna_sharing_2020].

-   Metadata can provide information on the structure of the data and how the data was collected.

-   The metadata may also outline or provide insights into the limitations of the data.

-   A visual inspection of your entire dataset can save time and solve easy-to-spot errors.

-   Help identify limitations

A simple way to visualize your data is to plot it on a map. This simple step can highlight coordinates with missing data or even inverted or erroneous coordinates (as points floating in the ocean while focusing on terrestrial species and vice versa).

Other tools, such as looking for only positive values (if that is what is expected) can also be used in this initial data familiarization step.

```{r}

```

## Precleaning (I think this part should be its own section)

Systematic data cleaning is required, however, some mistakes observed during the initial visualization of the data and metadata can be fixed straight away.

-   Typos

-   Ensure consistent capitalizations

-   Fix minor coordinates errors, such as inverted or badly formatted

-   Remove records with no coordinates

```{r}

```

## Notes 

Do you want to limit the data to museum records or to citizen data? Certain data providers have their own naming conventions... [Not sure if this is a good place for this]
