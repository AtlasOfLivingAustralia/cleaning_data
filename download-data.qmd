---
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---
```{r, setup, include=FALSE}

knitr::opts_chunk$set(
  comment = '', 
  fig.width = 6, 
  fig.height = 6,
  eval = FALSE,
  echo = FALSE,
  warning = FALSE
)

library(galah)
library(dplyr)
library(arrow)
```

# Download data {#sec-download-data} 

Once we have decided on our data scope, we can precede to download data. In this section, we will introduce a few common data infrastructures that offer open access biodiversity data and highlight some considerations when choosing one in the context of your data scope. We will then discuss some obstacles when consolidating data from multiple sources and the importance of metadata. 

## Where to get data from

### Global

The [Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/), an international network and data infrastructure, stores global biodiversity data from many sources around the world. GBIF consists of a series of ‘node’ organisations who collate biodiversity data from their own regions and countries, with GBIF acting as an overarching organisation to store data from all nodes. Users that are interested in obtaining data that has global coverage may want to download directly from GBIF. 

### Regional

Alternatively, using a *regional node* may be more relevant if your project is at a smaller scale. For example, the [Atlas of Living Australia (ALA)](www.ala.org.au) is the Australian node, [Sistema de Informação sobre a Biodiversidade Brasileira (SiBBr)](https://www.sibbr.gov.br/) is the Brazilian node, and [GBIF Sweden](https://www.gbif.se/) is the Swedish node. Living Atlases like the ALA ingest and aggregate data from a broad range of providers such as government monitoring programs, museums and herbaria, research projects and citizen science initiatives. 

To see what national and regional nodes exist, check out [this page](https://www.gbif.org/the-gbif-network).

Some regional nodes like the ALA and SiBBr use their own taxonomic backbone for classification, which might differ from the taxonomic backbone of other regional or global infrastructures like GBIF. These taxonomic classifications can be useful for classifying local flora and fauna, and accepted by experts of the region. It's worth being aware of what @sec-namingauthorities are used to error caused by classification later on.

### Data providers

If your project relates to data from a specific data provider, it might be best to download data directly from the source. For example, a common citizen science tool to collect species observations is [iNaturalist](https://www.inaturalist.org/). Downloading directly from the data provider may ensure you don't have any stray data from other sources.

<!-- this is a relatively small, uncomprehensive list. Do we want to expand to spatial data? Should we give more examples of websites or packages that contain these data? --> 


## How to download data

### Using taxonomic information

#### Naming authorities {#sec-namingauthorities}

Main point: It's worth being aware of what naming authority your taxonomic backbone uses. In some taxonomic groups, these can vary widely. Double checking your data after your download to make sure the classifications you expect are what you have will help prevent errors later on. Otherwise, you can re-code data with incorrect classification manually.

Now that you've chosen a naming authority you can use it to ensure consistency across your data set, but also check you haven't missed any species data from your download.


#### Searching by classification

Include the original name that was recorded by the data provider in your download. This is often referred to as `verbatimScientificName`. or vernacularname



```{r}
# Orchid as an example 
# Read in problem child data and show

# Group by species and count unique values for higher taxonomy
```

2.  Include synonyms of the species names you're interested in your download

```{r}
#| eval: true
#| echo: true
library(galah)
library(gt)

galah_call() |>
  galah_filter(year > 2019) |>
  atlas_counts() |>
  gt()
```


Many developers have created R packages to interact with each data infrastructures's API to aid access to biodiversity data. Here are a few examples, we recommend taking a look at each package's documentation to choose one that suits your project. 

- [`rgbif`](https://docs.ropensci.org/rgbif/) an interface to GBIF
- [`galah`](https://galah.ala.org.au/index.html)  an interface to a number living Atlases, as well as GBIF
- [`rinat`](https://docs.ropensci.org/rinat/) an interface to iNaturalist observations
- [`rebird`](https://docs.ropensci.org/rebird/) an interface with the eBird webservices.
- [`spocc`](https://docs.ropensci.org/spocc/) a R package to query and collect species occurrence data from various sources including [VertNet](https://github.com/ropensci/rvertnet), [iDigBio](http://www.idigbio.org/) and others.

One benefit of using `galah` is that enables users to acquire not only species occurrence records but also taxonomic information, or associated media such as images or sounds. Below we have included some code blocks for downloading occurrence data with `galah` from GBIF and a Spain node.

#### GBIF data via `galah`

First, we have to configure `galah`. This is where you supply your account credentials and set the atlas to a particular region. See `?galah_config` for more configuration options. You can [save these credentials](https://docs.ropensci.org/rgbif/articles/gbif_credentials.html) in your `.Renviron` so you don't have to enter it explicitly in code. 

We will be downloading all occurrences for the African Elephant from GBIF. This may take a while as it is around 12,000 records. Once downloaded, you can save the records locally in your desired format. For larger downloads, we recommend saving the data as `parquets` using `arrow::write_parquet`

```{r, echo = TRUE}
galah_config(email = Sys.getenv("ALA_EMAIL"),
             username = Sys.getenv("GBIF_USER"),
             password = Sys.getenv("GBIF_PWD"),
             atlas = "Global")

african_ele <- galah_call() %>% 
  galah_identify("Loxodonta africana") %>% 
  atlas_occurrences()

arrow::write_parquet(african_ele, "data/gbif/elephant")
```

```{r, echo = FALSE, eval = TRUE}
read_parquet("data/gbif/elephant")
```


#### Regional node via `galah`

In order to access data from the Australia node, we will need to reconfigure `galah` so that our query points to Australia. After that, we will download all records for the Pink Robin.

```{r, echo = TRUE}
galah_config(email = Sys.getenv("ALA_EMAIL"), 
             atlas = "Australia")

pink_robin <- galah_call() %>% 
  galah_identify("Petroica rodinogaster") %>% 
  atlas_occurrences
```

```{r}
arrow::write_parquet(pink_robin, "data/galah/pink_robin")
```

```{r, echo = FALSE, eval = TRUE}
read_parquet("data/galah/pink_robin")
```

### Using spatial information

<!-- Work in progess -->
<!-- Narrowing your results to a specific spatial area- using `galah_geolocate` or by regions etc, discussions around whether this is best to be placed in this chapter to assist with expaning scope has been raised.  -->
<!-- Read in shpfile, transform to polygon -->

#### Searching by region

One way to download data is by filtering to an area of interest using fields already in the ALA and {galah}.

```{r}
# Show how to download data in galah by finding a field (like by state/territory or smaller region)
```

#### Searching by bounding box

Another way to download data is by filtering to return observations within a bounding box. 

```{r}
# Show how to download data in galah with a bounding box
```

This can be useful when you want to be sure to include records on the border or just outside the border of a defined region (which might affect predictions of where species live in a model).

It can also be useful if you want to filter data yourself.

#### Searching with a shapefile

You can use a **shapefile** to filter observations. Shapefiles are [a file with points to make an outline, they can be simple or complex].

```{r}
# Show a wkt string constructed as a polygon & a count of points
# Show a complex shapefile polygon & a count of points
```

In R, using shapefiles requires using a package for handling spatial data like the {sf} package. [This is because the sf package helps transform & edit our shapefile to be plotted using longitude and latitude using a specified coordinate reference system - define.]

Here is a simple polygon we have constructed for a theoretical "site A". We use `st_as_sf()` and `st_set_crs()` from the convert our shapefile to a spatial object in R, then set its coordinate reference system. This allows us to plot this object with {ggplot2}.

```{r}
my_polygon <- tibble(site = "A",
       geometry = "POLYGON((131.36328125 -22.506468769126,135.23046875 -23.396716654542,134.17578125 -27.287832521411,127.40820312499 -26.661206402316,128.111328125 -21.037340349154,131.36328125 -22.506468769126))")

my_polygon_sf <- st_as_sf(my_polygon, wkt = "geometry") %>%
  st_set_crs(4326) 

ggplot() +
  geom_sf(data = my_polygon_sf)

galah_call() |>
  galah_identify("Perameles") |>
  galah_geolocate(wkt) |>
  atlas_species()
```

Using shapefiles allows us to return data for very specific shapes, and are useful for long-term analyses of observations in specific regions or areas.



## Choosing specific data columns

<!-- This shouldn't be its own section. This is a tool, not necessarily a topic. This information can be worked into another section, but definitely doesn't deserve its own heading -->

By default, `atlas_occurrences` will return a tibble with a selection of columns containing taxonomic and spatial data as well as other metadata. Alternatively, you can use `galah_select` to subset the columns that are relevant for your work. To see all available fields you can choose from:

```{r, echo = TRUE, eval = TRUE}
show_all(fields) 
```

Here, we will choose a smaller subsets of 8 columns to download for the Pink Robin

```{r, echo = TRUE}
project_fields <- c("recordID",
                    "eventDate",
                    "year", 
                    "basisOfRecord", 
                    "occurrenceStatus",
                    "scientificName",
                    "decimalLatitude",
                    "decimalLongitude")

pink_robin_projfields <- galah_call() %>% 
  galah_identify("Petroica rodinogaster") %>% 
  galah_select(project_fields) %>% 
  atlas_occurrences()
```

```{r}
arrow::write_parquet(pink_robin_projfields, "data/galah/probin_fields")
```

```{r, eval = TRUE}
read_parquet("data/galah/probin_fields")
```



## Refining your data download

<!-- I think this information belongs in the Data Scope chapter. The chapter might even be renamed to "Investigating data", or something. But breaking data down like this by year & data type is how a person checks what is available and makes decisions about what data to include or omit. This also introduces useful tools like `filter()` and `group_by()` + `count()` early on in the workflow  -->

Open access biodiversity data comes from many different data sources, (eg. government monitoring programs, museums, herbaria, research projects, citizen science apps). As such, data type and quality can vary considerably. For example, museums harbour older records that are associated with a preserved specimens. These data often contain lots of extra information (metadata) about a specific specimen and its location. Alternatively, data sourced from citizen science apps like iNaturalist or eBird are associated with images or sounds captured from a smart phone. These data might contain less metadata of each observation, but, thanks to phones' accurate location services, are quite accurate about the time and location of an observation. 

Refining your download query ensures higher quality data and also reduces the download size as many data infrastructures impose constraints to download size. Below we have illustrated how you can refine your query a few quality measures using `galah_filter`. 

#### By Year

Generally, old data records tend to be insufficient or less reliable as taxonomic knowledge and GPS tools were not readily available. For this reason, many users consider removing all occurrence records before a certain year to increase data precision [@gueta_quantifying_2016; @marsh_accounting_2022] . 

Choosing the year 'cut-off' is relatively arbitary, but the most commonly used year is 1945 [@zizka_no_2020; @fuhrding-potschkat_influence_2022], although some studies discard all data collected before 1990 [@gueta_quantifying_2016; @marsh_accounting_2022].

Here we will narrow the Pink Robin query from above to records after 1945 using `galah_filter`:

```{r, echo = TRUE}
pink_robin_post1945 <- galah_call() %>% 
  galah_identify("Petroica rodinogaster") %>% 
  galah_filter(year > 1945) %>% 
  atlas_occurrences()
```

#### Basis of record

Basis of record is a [Darwin Core term](https://dwc.tdwg.org/terms/#dwc:basisOfRecord) that refers to the specific nature of the occurrence record. It can be used to refine your data download and ensure consistency when consolidating data from multiple organisations [@fuhrding-potschkat_influence_2022]. 

There are 6 different classes for basis of record: 

- Living Specimen - a specimen that is alive, e.g. a living plant in a national park
- Preserved Specimen - a specimen that has been preserved, for example, a dried plant on an herbarium sheet 
- Fossil Specimen - a preserved specimen that is a fossil
- Material Sample - a genetic or environmental sample
- Material Citation - A reference to, or citation of, a specimen in scholarly publications, e.g a citation of a physical specimen in a scientific journal 
- Human Observation - an output of human observation process e.g. evidence of an occurrence taken from field notes or an occurrence without any physical evidence
- Machine Observation - An output of a machine observation process e.g. a photograph, a video, an audio recording, a remote sensing image or an occurrence record based on telemetry.

Depending on your data scope, it may be practical to limit data that can be traced to a physical specimen or observation [@godfree_implications_2021], which we do for the Pink Robin below

```{r, echo=TRUE}
tractable_records <- c("LIVING_SPECIMEN", 
                       "PRESERVED_SPECIMEN", 
                       "MATERIAL_SAMPLE", 
                       "MACHINE_OBSERVATION")

pink_robin_tractable <- galah_call() %>% 
  galah_identify("Petroica rodinogaster") %>% 
  galah_filter(basisOfRecord == tractable_records) %>% 
  atlas_occurrences()
```

#### Assertions

Data infrastructures use assertions to internally grade the quality, completeness and consistency of each occurrence record. Assertions take values of either 1 or 0, indicating the presence or absence of the data quality issue. Note that assertions may vary depending what atlas you have configured to. You can see the available assertions and their descriptions using:

```{r, eval = TRUE, echo=TRUE}
show_all("assertions") 
```

Once you have decided which assertions are important for your project you can further refine your download. To retrieve all the assertions for your query use `galah_select(group = "assertions")`

<!-- Explain:  -->
<!-- Assertions are logical flags, TRUE is presence of issue denoted in column name -->
<!-- User to choose which ones are relevant to their project and filter in query -->
<!-- For exclusions using multiple assertions (Currently throws errors, need to think carefully about != means for logical assertions...) -->

```{r}
probin_assertions <- galah_call() %>% 
  galah_identify("Petroica rodinogaster") %>% 
  galah_filter(basisOfRecord == tractable_records) %>% 
  galah_select(group = "assertions") %>% 
  atlas_occurrences() 

# Preview all the assertions
colnames(probin_assertions)

# A quick way to check which assertions contain TRUEs
probin_assertions |> select(-recordID) |> colSums()

# Requery for single assertion
probin_subset <- galah_call() %>% 
  galah_identify("Petroica rodinogaster") %>% 
  galah_filter(basisOfRecord == tractable_records,
               identificationIncorrect == FALSE) %>% 
  galah_select(group = "basic") %>% 
  atlas_counts() 


# For exclusions using multiple assertions [Currently throws errors, need to think carefully about != means for logical assertions...]
# assertions <- c("UNKNOWN_KINGDOM", "identificationIncorrect",
#                    "COORDINATE_PRECISION_MISMATCH", "MISSING_GEODETICDATUM")
# 
# galah_call() %>%
#   galah_identify("Petroica rodinogaster") %>%
#   galah_filter(basisOfRecord == tractable_records,
#                assertions != IA_assertions) %>%
#   atlas_counts()
```






 
 